{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import select\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score,\n",
    "    accuracy_score,\n",
    "    roc_curve,\n",
    "    auc,\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    matthews_corrcoef,\n",
    "    log_loss,\n",
    "    confusion_matrix\n",
    ")\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras import backend as k\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "def load_data(file_path):\n",
    "    \"\"\"\n",
    "    Lädt die Daten aus einer CSV-Datei und verarbeitet sie entsprechend den Anforderungen.\n",
    "    Erstellt eine neue Spalte 'Fluktuation', die 1 für 'Ausgeschieden' und 0 für andere Werte enthält,\n",
    "    und entfernt die Spalte 'Status'.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Der Dateipfad zur CSV-Datei.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Der vorverarbeitete DataFrame.\n",
    "    \"\"\"\n",
    "\n",
    "    # Daten laden\n",
    "    df = pd.read_csv(file_path, low_memory=False)\n",
    "\n",
    "    return df\n",
    "\n",
    "def preprocess_data(df, output_file=\"X_transformed.csv\"):\n",
    "    \"\"\"\n",
    "    Vorverarbeitung der Daten:\n",
    "    - Zielvariable \"Fluktuation\" ableiten\n",
    "    - Feature-Auswahl basierend auf Analyse\n",
    "    - One-Hot-Encoding für kategorische Features\n",
    "    - Klassenungleichgewicht mit SMOTE behandeln\n",
    "    - Anpassung von \"Fehlzeiten_Krankheitstage\" auf Basis von \"Abwesenheitsgrund\"\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): Der Original-Dataframe.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Original transformierte Features (X_transformed).\n",
    "        pd.DataFrame: Resampled Features (X_resampled).\n",
    "        np.ndarray: Zielvariable (y_resampled) nach SMOTE.\n",
    "        ColumnTransformer: Preprocessor für spätere Transformationen.\n",
    "    \"\"\"\n",
    "    # Mitarbeiter mit Status \"Ruhestand\" entfernen\n",
    "    df = df.copy()  # Vermeide Original zu modifizieren\n",
    "    df = df[df['Status'] != 'Ruhestand']\n",
    "    print(f\"Shape nach Entfernen von 'Ruhestand': {df.shape}\")\n",
    "\n",
    "    # Bereinige \"Fehlzeiten_Krankheitstage\", wenn der \"Abwesenheitsgrund\" nicht \"Krankheit\" ist\n",
    "    df['Fehlzeiten_Krankheitstage'] = df.apply(\n",
    "        lambda row: row['Fehlzeiten_Krankheitstage'] if row['Abwesenheitsgrund'] == \"Krankheit\" else 0,\n",
    "        axis=1\n",
    "    )\n",
    "    # Zielvariable \"Fluktuation\" ableiten\n",
    "    df['Fluktuation'] = df['Status'].apply(lambda x: 1 if x == \"Ausgeschieden\" else 0)\n",
    "\n",
    "    # Features kombinieren (manuelle Auswahl nach Analyse)\n",
    "    selected_features = [\n",
    "        'Jahr','Monat', 'Alter', 'Überstunden', 'Fehlzeiten_Krankheitstage',\n",
    "        'Gehalt', 'Zufriedenheit', 'Fortbildungskosten',\n",
    "        'Position', 'Geschlecht', 'Standort',\n",
    "        'Arbeitszeitmodell', 'Verheiratet',\n",
    "        'Kinder', 'Job Role Progression', 'Job Level', 'Tenure'\n",
    "    ]\n",
    "    X = df[selected_features]\n",
    "    y = df['Fluktuation']\n",
    "\n",
    "    print(f\"Shape von X: {X.shape}\")\n",
    "    print(f\"Shape von y: {y.shape}\")\n",
    "\n",
    "    # Kategorische und numerische Spalten erkennen\n",
    "    categorical_columns = X.select_dtypes(include='object').columns\n",
    "    numerical_columns = X.select_dtypes(exclude='object').columns\n",
    "\n",
    "    # Preprocessor definieren\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', 'passthrough', numerical_columns),\n",
    "            ('cat', OneHotEncoder(drop='first', sparse_output=False), categorical_columns)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Features transformieren\n",
    "    X_transformed = preprocessor.fit_transform(X)\n",
    "\n",
    "    # Neue Spaltennamen nach Transformation\n",
    "    transformed_columns = list(numerical_columns) + \\\n",
    "                          list(preprocessor.named_transformers_['cat'].get_feature_names_out(categorical_columns))\n",
    "\n",
    "    # In DataFrame zurückschreiben\n",
    "    X_transformed = pd.DataFrame(X_transformed, columns=transformed_columns, index=X.index)\n",
    "\n",
    "    # Synchronisation von df mit X_transformed sicherstellen\n",
    "    # Prüfe die Indizes, um sicherzustellen, dass nur relevante Zeilen enthalten sind\n",
    "    if not X_transformed.index.equals(df.index):\n",
    "        print(\"Index nicht identisch! Sync wird vorgenommen.\")\n",
    "        df = df.loc[X_transformed.index]  # Synchronisierung basierend auf dem Index von X_transformed\n",
    "\n",
    "    # SMOTE für Klassenungleichgewicht\n",
    "    smote = SMOTE(random_state=42)\n",
    "    X_resampled, y_resampled = smote.fit_resample(X_transformed, y)\n",
    "\n",
    "    # Zurück als DataFrame für interpretierbare Spalten\n",
    "    X_resampled = pd.DataFrame(X_resampled, columns=X_transformed.columns)\n",
    "\n",
    "    # Transformierte Daten X_transformed als CSV speichern\n",
    "    X_transformed.to_csv(output_file, index=False)  # Speichert die Datei\n",
    "    print(f\"Transformierte Daten wurden gespeichert unter '{output_file}'\")\n",
    "\n",
    "    return df, X_transformed, X_resampled, y_resampled, preprocessor\n",
    "\n",
    "def split_and_scale(X_resampled, y_resampled, test_size=0.2, scaler_file=\"Models/scaler.pkl\"):\n",
    "    \"\"\"\n",
    "    Teilt die Daten in Trainings- und Testsets und skaliert sie. Speichert den Scaler für spätere Skalierungen.\n",
    "\n",
    "    Args:\n",
    "        X_resampled (np.ndarray): Features nach Resampling.\n",
    "        y_resampled (np.ndarray): Zielvariable nach Resampling.\n",
    "        test_size (float): Anteil der Testdaten.\n",
    "        scaler_file (str): Dateiname für das Speichern des Scalers.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Skalierte Trainingsdaten (X_train_scaled).\n",
    "        pd.DataFrame: Skalierte Testdaten (X_test_scaled).\n",
    "        np.ndarray: Trainings-Zielvariablen.\n",
    "        np.ndarray: Test-Zielvariablen.\n",
    "    \"\"\"\n",
    "\n",
    "    # Daten aufteilen\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_resampled, y_resampled, test_size=test_size, random_state=42, stratify=y_resampled\n",
    "    )\n",
    "\n",
    "    # Daten skalieren\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    # Scaler speichern\n",
    "    joblib.dump(scaler, scaler_file)  # Speichert den Scaler in einer Datei\n",
    "    print(f\"Scaler wurde gespeichert unter '{scaler_file}'\")\n",
    "\n",
    "    # Daten zurück in DataFrame konvertieren\n",
    "    X_train = pd.DataFrame(X_train_scaled, columns=X_resampled.columns)\n",
    "    X_test = pd.DataFrame(X_test_scaled, columns=X_resampled.columns)\n",
    "\n",
    "    return X_train, X_test, y_train, y_test, scaler\n",
    "\n",
    "def reduce_dimensions(X_train, X_test, X_resampled, n_components=0.95):\n",
    "    \"\"\"\n",
    "    Führt PCA durch, während Spaltennamen erhalten bleiben, falls DataFrames genutzt werden.\n",
    "\n",
    "    Args:\n",
    "        X_train (pd.DataFrame oder np.ndarray): Trainingsdatensatz.\n",
    "        X_test (pd.DataFrame oder np.ndarray): Testdatensatz.\n",
    "        X_full (pd.DataFrame oder np.ndarray): Gesamtdatensatz.\n",
    "        n_components (float oder int): Anzahl der Hauptkomponenten oder Anteil der erklärten Varianz.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, PCA]: PCA-Transformation als DataFrames und das PCA-Modell.\n",
    "    \"\"\"\n",
    "    X_full = X_resampled\n",
    "    try:\n",
    "        # Prüfen, ob Eingaben DataFrames sind\n",
    "        is_dataframe = isinstance(X_train, pd.DataFrame)\n",
    "\n",
    "        if is_dataframe:\n",
    "            # Spaltennamen und Index speichern\n",
    "            if not (X_train.columns.equals(X_test.columns) and X_train.columns.equals(X_full.columns)):\n",
    "                raise ValueError(\"Die Spalten der Eingaben (X_train, X_test, X_full) sind inkonsistent.\")\n",
    "\n",
    "            X_train_columns = X_train.columns\n",
    "            index_train, index_test, index_full = X_train.index, X_test.index, X_full.index\n",
    "        else:\n",
    "            # Sicherstellen, dass alle Eingaben NumPy-Arrays sind\n",
    "            if not isinstance(X_train, np.ndarray) or not isinstance(X_test, np.ndarray) or not isinstance(X_full,\n",
    "                                                                                                           np.ndarray):\n",
    "                raise ValueError(\"Alle Eingabedaten müssen entweder 'pd.DataFrame' oder 'np.ndarray' sein.\")\n",
    "            if X_train.shape[1] != X_test.shape[1] or X_train.shape[1] != X_full.shape[1]:\n",
    "                raise ValueError(\"Alle Eingabedatensätze müssen die gleiche Anzahl von Spalten haben.\")\n",
    "\n",
    "        # PCA initialisieren und Trainingsdatensatz fitten\n",
    "        pca = PCA(n_components=n_components, svd_solver='full')\n",
    "        X_train_pca = pca.fit_transform(X_train)\n",
    "        X_test_pca = pca.transform(X_test)\n",
    "        X_full_pca = pca.transform(X_full)\n",
    "\n",
    "        # Debugging der erklärten Varianz (nur bei Verhältnis)\n",
    "        if isinstance(n_components, float):\n",
    "            variance_explained = np.sum(pca.explained_variance_ratio_) * 100\n",
    "            print(f\"PCA erklärt {variance_explained:.2f}% der Varianz durch {pca.n_components_} Komponenten.\")\n",
    "\n",
    "        print(f\"PCA erfolgreich abgeschlossen.\")\n",
    "        print(f\"- Originaldimensionen vor PCA: {X_train.shape[1]}\")\n",
    "        print(f\"- Reduzierte Dimensionen nach PCA: {X_train_pca.shape[1]}\")\n",
    "\n",
    "        # Falls DataFrame, Daten in DataFrame zurückgeben\n",
    "        if is_dataframe:\n",
    "            pca_columns = [f\"PCA_{i + 1}\" for i in range(X_train_pca.shape[1])]\n",
    "            X_train_pca = pd.DataFrame(X_train_pca, columns=pca_columns, index=index_train)\n",
    "            X_test_pca = pd.DataFrame(X_test_pca, columns=pca_columns, index=index_test)\n",
    "            X_full_pca = pd.DataFrame(X_full_pca, columns=pca_columns, index=index_full)\n",
    "\n",
    "        return X_train_pca, X_test_pca, X_full_pca, pca\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Fehler während der PCA-Berechnung: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def model_selection():\n",
    "    \"\"\"\n",
    "    Ermöglicht dem Benutzer, Modelle für die Analyse auszuwählen. Dabei können entweder\n",
    "    spezifische Modelle oder alle Modelle ausgewählt werden. Wenn der Benutzer innerhalb\n",
    "    von 10 Sekunden keine Auswahl trifft, werden automatisch alle Modelle ausgewählt.\n",
    "\n",
    "    Returns:\n",
    "        include_models (list): Liste der auszuwählenden Modelle.\n",
    "    \"\"\"\n",
    "    # Mapping für Modelle\n",
    "    model_mapping = {\n",
    "        1: \"Logistic Regression\",\n",
    "        2: \"Random Forest\",\n",
    "        3: \"XGBoost\",\n",
    "        4: \"LightGBM\",\n",
    "        5: \"Neural Network\"\n",
    "    }\n",
    "\n",
    "    print(\"\\nBitte wählen Sie Modelle für die Analyse aus:\")\n",
    "    print(\"1: Logistic Regression\")\n",
    "    print(\"2: Random Forest\")\n",
    "    print(\"3: XGBoost\")\n",
    "    print(\"4: LightGBM\")\n",
    "    print(\"5: Neural Network\")\n",
    "    print(\"6: Alle Modelle\")\n",
    "\n",
    "    def input_with_timeout(prompt, timeout=0.10):\n",
    "        \"\"\"\n",
    "        Funktion, die Benutzereingaben mit Timeout verarbeitet. Wenn der Benutzer nicht innerhalb\n",
    "        der angegebenen Zeit reagiert, wird None zurückgegeben.\n",
    "        \"\"\"\n",
    "        print(prompt, end='', flush=True)  # Eingabeaufforderung anzeigen\n",
    "        inputs, _, _ = select.select([sys.stdin], [], [], timeout)\n",
    "        if inputs:\n",
    "            return sys.stdin.readline().strip()  # Eingabe lesen und zurückgeben\n",
    "        else:\n",
    "            print(\"\\nTimeout abgelaufen. Keine Eingabe erkannt.\")\n",
    "            return None\n",
    "\n",
    "    # Warten auf Benutzereingabe mit Timeout\n",
    "    selected_input = input_with_timeout(\n",
    "        \"Geben Sie die Nummern der gewünschten Modelle durch Kommas getrennt ein (z. B.: 1,3,5 oder 6 für alle Modelle):\\n\",\n",
    "        timeout=0.10  # Timeout in Sekunden\n",
    "    )\n",
    "\n",
    "    # Wenn keine Eingabe erfolgt ist, werden alle Modelle gewählt\n",
    "    if selected_input is None:\n",
    "        print(\"\\nKeine Auswahl getroffen. Es werden automatisch alle Modelle ausgewählt.\")\n",
    "        return list(model_mapping.values())  # Gibt alle Modelnamen zurück\n",
    "\n",
    "    # Verarbeiten der Benutzereingabe\n",
    "    try:\n",
    "        selected_numbers = [int(n) for n in selected_input.split(\",\")]  # Benutzereingaben in Liste umwandeln\n",
    "    except ValueError:\n",
    "        raise ValueError(\"Ungültige Eingabe. Bitte geben Sie gültige Modellnummern ein (z. B.: 1,2 oder 6).\")\n",
    "\n",
    "    if 6 in selected_numbers:  # 'Alle Modelle' wurde ausgewählt\n",
    "        include_models = list(model_mapping.values())  # Alle Modelle zurückgeben\n",
    "    else:\n",
    "        # Nur die ausgewählten Modelle zusammenstellen\n",
    "        include_models = [model_mapping[n] for n in selected_numbers if n in model_mapping]\n",
    "\n",
    "    if not include_models:  # Sicherstellen, dass mindestens ein Modell ausgewählt wurde\n",
    "        raise ValueError(\"Keine gültigen Modelle ausgewählt. Bitte wählen Sie mindestens ein Modell.\")\n",
    "\n",
    "    return include_models\n",
    "\n",
    "def get_user_choice(models_dir, timeout=0.10):\n",
    "    \"\"\"\n",
    "    Fragt den Benutzer, ob gespeicherte Modelle verwendet oder neue Modelle trainiert werden sollen.\n",
    "\n",
    "    Args:\n",
    "        models_dir (str): Verzeichnis, in dem die Modelle gespeichert werden.\n",
    "        timeout (int): Zeit in Sekunden, die der Benutzer hat, um eine Eingabe zu tätigen.\n",
    "\n",
    "    Returns:\n",
    "        bool: True, wenn gespeicherte Modelle verwendet werden sollen und diese existieren, sonst False.\n",
    "    \"\"\"\n",
    "    def input_with_timeout(prompt, timeout=0.10):\n",
    "        print(prompt, end='', flush=True)\n",
    "        inputs, _, _ = select.select([sys.stdin], [], [], timeout)\n",
    "        if inputs:\n",
    "            return sys.stdin.readline().strip().lower()  # Eingabe lesen\n",
    "        else:\n",
    "            print(\"\\nTimeout abgelaufen. Standard: 'y' (gespeicherte Modelle verwenden).\")\n",
    "            return \"y\"  # Standardoption ist jetzt \"y\"\n",
    "\n",
    "    # Benutzer auswählen lassen\n",
    "    user_choice = input_with_timeout(\n",
    "        \"Verwenden Sie gespeicherte Modelle? [y/n] (Standard: y): \", timeout\n",
    "    )\n",
    "\n",
    "    # Prüfung, ob Modelle existieren\n",
    "    def models_exist(models_dir):\n",
    "        # Sucht nach vorhandenen Modelldateien im `model_dir`\n",
    "        for file in os.listdir(models_dir):\n",
    "            if file.endswith(\".pkl\") or file.endswith(\".json\") or file.endswith(\".keras\"):\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    # Wenn Benutzeroption 'y' angibt, prüfen, ob Modelle vorhanden sind\n",
    "    if user_choice == \"y\":\n",
    "        if models_exist(models_dir):\n",
    "            print(\"Gespeicherte Modelle werden verwendet.\")\n",
    "            return True\n",
    "        else:\n",
    "            print(\"Keine gespeicherten Modelle gefunden. Neues Training wird gestartet.\")\n",
    "            return False\n",
    "    elif user_choice == \"n\":\n",
    "        print(\"Neue Modelle werden trainiert.\")\n",
    "        return False\n",
    "    else:\n",
    "        print(\"Ungültige Eingabe. Standard: 'y' (gespeicherte Modelle verwenden).\")\n",
    "        if models_exist(models_dir):\n",
    "            print(\"Gespeicherte Modelle werden verwendet.\")\n",
    "            return True\n",
    "        else:\n",
    "            print(\"Keine gespeicherten Modelle gefunden. Neues Training wird gestartet.\")\n",
    "            return False\n",
    "\n",
    "def train_models(X_train, X_test, y_train, y_test, include_models, use_saved_models, models_dir):\n",
    "    \"\"\"\n",
    "    Trainiert dynamisch Modelle basierend auf der Auswahl `include_models`.\n",
    "\n",
    "    Args:\n",
    "        X_train, X_test, y_train, y_test: Trainings- und Test-Daten.\n",
    "        include_models (list): Liste der zu trainierenden Modelle.\n",
    "\n",
    "    Returns:\n",
    "        list: Liste von Ergebnissen (Scores und Modelle für jedes Modell).\n",
    "    \"\"\"\n",
    "    trained_models_results = []\n",
    "\n",
    "    if \"Logistic Regression\" in include_models:\n",
    "        try:\n",
    "            result = train_logistic_regression(X_train, X_test, y_train, y_test,use_saved_models, models_dir)\n",
    "            if \"model_name\" in result:  # Sicherstellen, dass das Ergebnis valide ist\n",
    "                trained_models_results.append(result)\n",
    "            else:\n",
    "                print(f\"Warnung: Logistic Regression hat kein valides Ergebnis zurückgegeben.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Fehler beim Training der Logistic Regression: {e}\")\n",
    "\n",
    "    if \"Random Forest\" in include_models:\n",
    "        try:\n",
    "            result = train_random_forest(X_train, X_test, y_train, y_test, use_saved_models, models_dir)\n",
    "            if \"model_name\" in result:\n",
    "                trained_models_results.append(result)\n",
    "            else:\n",
    "                print(f\"Warnung: Random Forest hat kein valides Ergebnis zurückgegeben.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Fehler beim Training des Random Forest: {e}\")\n",
    "\n",
    "    if \"XGBoost\" in include_models:\n",
    "        try:\n",
    "            result = train_xgboost(X_train, X_test, y_train, y_test, use_saved_models, models_dir)\n",
    "            if \"model_name\" in result:\n",
    "                trained_models_results.append(result)\n",
    "            else:\n",
    "                print(f\"Warnung: XGBoost hat kein valides Ergebnis zurückgegeben.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Fehler beim Training von XGBoost: {e}\")\n",
    "\n",
    "    if \"LightGBM\" in include_models:\n",
    "        try:\n",
    "            result = train_lightgbm(X_train, X_test, y_train, y_test, use_saved_models, models_dir)\n",
    "            if \"model_name\" in result:\n",
    "                trained_models_results.append(result)\n",
    "            else:\n",
    "                print(f\"Warnung: LightGBM hat kein valides Ergebnis zurückgegeben.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Fehler beim Training des LightGBM: {e}\")\n",
    "\n",
    "    if \"Neural Network\" in include_models:\n",
    "        try:\n",
    "            result = train_neural_network(X_train, X_test, y_train, y_test, use_saved_models, models_dir)\n",
    "            if \"model_name\" in result:\n",
    "                trained_models_results.append(result)\n",
    "            else:\n",
    "                print(f\"Warnung: Neural Network hat kein valides Ergebnis zurückgegeben.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Fehler beim Training des Neural Networks: {e}\")\n",
    "\n",
    "    return trained_models_results\n",
    "\n",
    "def train_logistic_regression(X_train, X_test, y_train, y_test, use_saved_models, models_dir, pca=None):\n",
    "    \"\"\"\n",
    "    Trainiert oder lädt Logistic Regression mit optionaler PCA und behandelt Dimension-Mismatches.\n",
    "    \"\"\"\n",
    "    model_file = os.path.join(models_dir, 'logistic_regression_with_pca.pkl')\n",
    "\n",
    "    try:\n",
    "        # Verzeichnis für Modelle erstellen, falls es nicht existiert\n",
    "        if not os.path.exists(models_dir):\n",
    "            os.makedirs(models_dir)\n",
    "\n",
    "        # Prüfen, ob ein gespeichertes Modell geladen werden soll\n",
    "        model = None\n",
    "        if use_saved_models and os.path.exists(model_file):\n",
    "            print(\"[INFO] Lade gespeichertes Logistic Regression Modell...\")\n",
    "            model, loaded_pca = joblib.load(model_file)\n",
    "\n",
    "            # Prüfen, ob PCA geladen wurde und sollte verwendet werden\n",
    "            if pca is None and loaded_pca is not None:\n",
    "                print(\"[INFO] Verwende gespeichertes PCA-Objekt.\")\n",
    "                pca = loaded_pca\n",
    "\n",
    "            # Prüfen, ob die Trainingsdaten zu den Modell-Features passen\n",
    "            if X_train.shape[1] != model.n_features_in_:\n",
    "                print(\n",
    "                    f\"[WARNUNG] Die Anzahl der Features im Modell ({model.n_features_in_}) stimmt nicht mit den Eingabedaten überein ({X_train.shape[1]}).\")\n",
    "                print(\"[INFO] Versuche automatische Anpassung der Features...\")\n",
    "\n",
    "                # Fehlende Spalten ergänzen\n",
    "                missing_columns = set(model.feature_names_in_) - set(X_train.columns)\n",
    "                for col in missing_columns:\n",
    "                    X_train[col] = 0\n",
    "                    X_test[col] = 0\n",
    "\n",
    "                # Zusätzliche Spalten entfernen\n",
    "                X_train = X_train[model.feature_names_in_]\n",
    "                X_test = X_test[model.feature_names_in_]\n",
    "\n",
    "        # Anwenden von PCA, falls vorhanden\n",
    "        if pca is not None:\n",
    "            print(\"[INFO] PCA auf Eingabedaten anwenden...\")\n",
    "            if hasattr(pca, \"n_components_\"):\n",
    "                if pca.n_components_ != X_train.shape[1] and model is not None:\n",
    "                    raise ValueError(\n",
    "                        f\"[FEHLER] PCA erzeugt {pca.n_components_} Features, aber das Modell erwartet {model.n_features_in_} Features.\")\n",
    "            X_train = pca.transform(X_train)\n",
    "            X_test = pca.transform(X_test)\n",
    "\n",
    "        # Falls kein Modell geladen wurde, Training starten\n",
    "        if model is None:\n",
    "            print(\"[INFO] Kein gespeichertes Modell gefunden. Starte neues Training...\")\n",
    "            param_grid = {\n",
    "                'solver': ['lbfgs'],\n",
    "                'C': [0.1, 1.0, 10],\n",
    "                'class_weight': ['balanced']\n",
    "            }\n",
    "            logreg_pipeline = GridSearchCV(\n",
    "                LogisticRegression(max_iter=500, random_state=42),\n",
    "                param_grid=param_grid,\n",
    "                scoring='roc_auc',\n",
    "                cv=3,\n",
    "                n_jobs=-1\n",
    "            )\n",
    "\n",
    "            # Training des Modells\n",
    "            print(\"[INFO] Starte GridSearchCV-Suche für Logistic Regression...\")\n",
    "            logreg_pipeline.fit(X_train, y_train)\n",
    "            model = logreg_pipeline.best_estimator_\n",
    "            print(\"[INFO] Training erfolgreich abgeschlossen.\")\n",
    "\n",
    "            # Speichern der Spaltennamen, um Dimension-Mismatches in Zukunft zu vermeiden\n",
    "            if isinstance(X_train, pd.DataFrame):\n",
    "                model.feature_names_in_ = X_train.columns\n",
    "\n",
    "            # Modell und PCA-Objekt speichern\n",
    "            joblib.dump((model, pca), model_file)\n",
    "            print(f\"[INFO] Modell gespeichert: {model_file}\")\n",
    "\n",
    "        # Vorhersagen und Performancemetriken berechnen\n",
    "        print(\"[INFO] Berechne Vorhersagen und Metriken...\")\n",
    "        y_train_proba = model.predict_proba(X_train)[:, 1]\n",
    "        y_test_proba = model.predict_proba(X_test)[:, 1]\n",
    "        threshold = 0.5  # Standardmäßiger Schwellenwert\n",
    "        y_train_pred = (y_train_proba > threshold).astype(int)\n",
    "        y_test_pred = (y_test_proba > threshold).astype(int)\n",
    "\n",
    "        # Performance berechnen\n",
    "        accuracy = accuracy_score(y_test, y_test_pred)\n",
    "        precision = precision_score(y_test, y_test_pred, zero_division=0)\n",
    "        recall = recall_score(y_test, y_test_pred, zero_division=0)\n",
    "        f1 = f1_score(y_test, y_test_pred, zero_division=0)\n",
    "        roc_auc = roc_auc_score(y_test, y_test_proba)\n",
    "\n",
    "        result = {\n",
    "            \"model_name\": \"Logistic Regression\",\n",
    "            \"trained_model\": model,\n",
    "            \"train_accuracy\": accuracy_score(y_train, y_train_pred),\n",
    "            \"test_accuracy\": accuracy,\n",
    "            \"roc_auc_score\": roc_auc,\n",
    "            \"f1_score\": f1,\n",
    "            \"precision\": precision,\n",
    "            \"recall\": recall,\n",
    "            \"y_test_proba\": y_test_proba\n",
    "        }\n",
    "\n",
    "        # Metriken auf der Konsole ausgeben\n",
    "        print(f\"\\nTrainings-Genauigkeit: {result['train_accuracy']:.4f}\")\n",
    "        print(f\"Test-Genauigkeit: {result['test_accuracy']:.4f}\")\n",
    "        print(f\"ROC-AUC: {result['roc_auc_score']:.4f}\")\n",
    "        print(f\"F1-Score: {result['f1_score']:.4f}\")\n",
    "        print(f\"Präzision: {result['precision']:.4f}\")\n",
    "        print(f\"Recall: {result['recall']:.4f}\")\n",
    "        print(\"=\" * 24)\n",
    "\n",
    "        return result\n",
    "\n",
    "    except FileNotFoundError as fnfe:\n",
    "        print(f\"[FEHLER] Speicherpfad oder Datei nicht gefunden: {fnfe}\")\n",
    "        raise\n",
    "\n",
    "    except ValueError as ve:\n",
    "        print(f\"[FEHLER] Ungültige Eingabedaten oder Modelle: {ve}\")\n",
    "        raise\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[UNERWARTETER FEHLER] {e}\")\n",
    "        raise\n",
    "\n",
    "def train_random_forest(X_train, X_test, y_train, y_test, use_saved_models, models_dir):\n",
    "    \"\"\"\n",
    "    Trainiert einen Random Forest Classifier mit anpassbarem Threshold.\n",
    "\n",
    "    Args:\n",
    "        X_train, X_test, y_train, y_test: Trainings- und Test-Daten.\n",
    "        use_saved_models: Flag, ob gespeicherte Modelle verwendet werden sollen.\n",
    "        models_dir: Pfad zum Speichern der Modelle.\n",
    "\n",
    "    Returns:\n",
    "        dict: Ergebnisdaten, einschließlich trainiertem Modell, Scores und Predictions.\n",
    "    \"\"\"\n",
    "    # Sicherstellen, dass X_train und X_test DataFrames mit identischen Spaltennamen sind\n",
    "    if isinstance(X_train, pd.DataFrame) and isinstance(X_test, pd.DataFrame):\n",
    "        if list(X_train.columns) != list(X_test.columns):\n",
    "            raise ValueError(\"Die Spalten von X_train und X_test stimmen nicht überein.\")\n",
    "    else:\n",
    "        raise TypeError(\"X_train und X_test müssen DataFrames sein, um Konsistenz sicherzustellen.\")\n",
    "\n",
    "    model_file = os.path.join(models_dir, 'random_forest_model.pkl')\n",
    "\n",
    "    # Prüfen, ob ein gespeichertes Modell vorhanden ist\n",
    "    if use_saved_models and os.path.exists(model_file):\n",
    "        print(f\"\\nGespeichertes Random Forest-Modell gefunden. Laden...\")\n",
    "        model = joblib.load(model_file)\n",
    "    else:\n",
    "        print(f\"\\nTraining für Random Forest startet...\")\n",
    "        model = RandomForestClassifier(class_weight='balanced', random_state=42)\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # Modell speichern\n",
    "        joblib.dump(model, model_file)\n",
    "        print(f\"Random Forest-Modell gespeichert als '{model_file}'\")\n",
    "\n",
    "    # Wahrscheinlichkeiten berechnen\n",
    "    y_train_proba = model.predict_proba(X_train)[:, 1]\n",
    "    y_test_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    # Schwellenwert (Threshold) für Klassifikation anpassen\n",
    "    threshold = 0.5  # Sie können diesen Wert anpassen\n",
    "    y_train_pred = (y_train_proba > threshold).astype(int)\n",
    "    y_test_pred = (y_test_proba > threshold).astype(int)\n",
    "\n",
    "    # Zusätzliche Bewertungsmethoden berechnen\n",
    "    results = {\n",
    "        \"model_name\": \"Random Forest\",\n",
    "        \"trained_model\": model,\n",
    "        \"train_accuracy\": accuracy_score(y_train, y_train_pred),\n",
    "        \"test_accuracy\": accuracy_score(y_test, y_test_pred),\n",
    "        \"roc_auc_score\": roc_auc_score(y_test, y_test_proba),\n",
    "        \"f1_score\": f1_score(y_test, y_test_pred, zero_division=0),\n",
    "        \"precision\": precision_score(y_test, y_test_pred, zero_division=0),\n",
    "        \"recall\": recall_score(y_test, y_test_pred, zero_division=0),\n",
    "        \"y_test_proba\": y_test_proba  # Falls Wahrscheinlichkeiten gespeichert werden sollen\n",
    "    }\n",
    "\n",
    "    # Bewertungsergebnisse ausgeben\n",
    "    print(f\"\\nTrainings-Genauigkeit: {results['train_accuracy']:.4f}\")\n",
    "    print(f\"Test-Genauigkeit: {results['test_accuracy']:.4f}\")\n",
    "    print(f\"ROC-AUC: {results['roc_auc_score']:.4f}\")\n",
    "    print(f\"F1-Score: {results['f1_score']:.4f}\")\n",
    "    print(f\"Präzision: {results['precision']:.4f}\")\n",
    "    print(f\"Recall: {results['recall']:.4f}\")\n",
    "    print(\"=\" * 24)\n",
    "\n",
    "    return results\n",
    "\n",
    "def train_xgboost(X_train, X_test, y_train, y_test, use_saved_models, models_dir):\n",
    "    \"\"\"\n",
    "    Trainiert einen XGBoost Classifier.\n",
    "\n",
    "    Args:\n",
    "        X_train, X_test, y_train, y_test: Trainings- und Test-Daten.\n",
    "        use_saved_models (bool): Ob ein gespeichertes Modell verwendet werden soll.\n",
    "        models_dir (str): Verzeichnis, in dem Modelle gespeichert werden sollen.\n",
    "\n",
    "    Returns:\n",
    "        dict: Ergebnisdaten, einschließlich trainiertem Modell, Scores und Predictions.\n",
    "    \"\"\"\n",
    "    # Datenvalidierung\n",
    "    if X_train is None or X_test is None or y_train is None or y_test is None:\n",
    "        raise ValueError(\"Trainings- oder Testdaten sind nicht korrekt geladen. Bitte überprüfen!\")\n",
    "    if len(X_train) == 0 or len(X_test) == 0 or len(y_train) == 0 or len(y_test) == 0:\n",
    "        raise ValueError(\"Trainings- oder Testdaten sind leer. Bitte überprüfen!\")\n",
    "\n",
    "    model_file = os.path.join(models_dir, 'xgboost_model.json')\n",
    "    model = None\n",
    "\n",
    "    # Prüfen, ob ein gespeichertes Modell vorhanden ist\n",
    "    if use_saved_models and os.path.exists(model_file):\n",
    "        try:\n",
    "            print(f\"\\nGespeichertes XGBoost-Modell gefunden. Laden...\")\n",
    "            model = xgb.XGBClassifier()\n",
    "            model.load_model(model_file)\n",
    "            # Sicherstellen, dass das geladene Modell verwendbar ist\n",
    "            if not hasattr(model, 'classes_'):\n",
    "                print(\"Warnung: Das geladene Modell scheint unvollständig zu sein. Neu-Training notwendig.\")\n",
    "                model = None\n",
    "        except Exception as e:\n",
    "            print(f\"Fehler beim Laden des gespeicherten Modells: {e}\")\n",
    "            model = None\n",
    "\n",
    "    # Trainieren, falls kein gültiges Modell geladen wurde\n",
    "    if model is None:\n",
    "        print(f\"\\nTraining für XGBoost startet...\")\n",
    "        model = xgb.XGBClassifier(eval_metric='logloss', random_state=42)\n",
    "        try:\n",
    "            model.fit(X_train, y_train)\n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"Fehler beim Trainieren des Modells: {e}\")\n",
    "\n",
    "        # Modell speichern\n",
    "        try:\n",
    "            model.save_model(model_file)\n",
    "            print(f\"XGBoost-Modell gespeichert als '{model_file}'\")\n",
    "        except Exception as e:\n",
    "            print(f\"Fehler beim Speichern des Modells: {e}\")\n",
    "\n",
    "    # Vorhersagen (Probabilistisch und Klassifikation)\n",
    "    try:\n",
    "        y_train_proba = model.predict_proba(X_train)[:, 1]  # Wahrscheinlichkeiten für Klasse 1\n",
    "        y_test_proba = model.predict_proba(X_test)[:, 1]\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Fehler bei probabilistischen Vorhersagen: {e}\")\n",
    "\n",
    "    # Schwellenwert (Threshold) für Klassifikation anpassen\n",
    "    threshold = 0.5  # Standardwert, kann angepasst werden\n",
    "    y_train_pred = (y_train_proba > threshold).astype(int)\n",
    "    y_test_pred = (y_test_proba > threshold).astype(int)\n",
    "\n",
    "    # ROC-AUC prüfen (Nur falls `predict_proba` verfügbar ist)\n",
    "    roc_auc = roc_auc_score(y_test, y_test_proba) if hasattr(model, \"predict_proba\") else None\n",
    "\n",
    "    # Evaluations-Metriken\n",
    "    results = {\n",
    "        \"model_name\": \"XGBoost\",\n",
    "        \"trained_model\": model,\n",
    "        \"train_accuracy\": accuracy_score(y_train, y_train_pred),\n",
    "        \"test_accuracy\": accuracy_score(y_test, y_test_pred),\n",
    "        \"roc_auc_score\": roc_auc,\n",
    "        \"f1_score\": f1_score(y_test, y_test_pred, zero_division=0),\n",
    "        \"precision\": precision_score(y_test, y_test_pred, zero_division=0),\n",
    "        \"recall\": recall_score(y_test, y_test_pred, zero_division=0),\n",
    "        \"y_test_proba\": y_test_proba  # Probabilistische Vorhersagen (optional für spätere Auswertungen)\n",
    "    }\n",
    "\n",
    "    # Bewertungsergebnisse ausgeben\n",
    "    print(f\"\\nTrainings-Genauigkeit: {results['train_accuracy']:.4f}\")\n",
    "    print(f\"Test-Genauigkeit: {results['test_accuracy']:.4f}\")\n",
    "    if roc_auc is not None:\n",
    "        print(f\"ROC-AUC: {results['roc_auc_score']:.4f}\")\n",
    "    print(f\"F1-Score: {results['f1_score']:.4f}\")\n",
    "    print(f\"Präzision: {results['precision']:.4f}\")\n",
    "    print(f\"Recall: {results['recall']:.4f}\")\n",
    "    print(\"=\" * 24)\n",
    "\n",
    "    return results\n",
    "\n",
    "def train_lightgbm(X_train, X_test, y_train, y_test, use_saved_models, models_dir):\n",
    "    \"\"\"\n",
    "    Trainiert einen LightGBM Classifier, speichert das Modell und die Feature-Namen lokal.\n",
    "    Falls ein gespeichertes Modell vorhanden ist, wird es geladen.\n",
    "\n",
    "    Args:\n",
    "        X_train, X_test, y_train, y_test: Trainings- und Test-Daten.\n",
    "        use_saved_models: Flag, ob gespeicherte Modelle verwendet werden sollen.\n",
    "        models_dir: Speicherpfad für das Modell.\n",
    "\n",
    "    Returns:\n",
    "        dict: Ergebnisdaten, inklusive trainiertem Modell, Scores und Predictions.\n",
    "    \"\"\"\n",
    "\n",
    "    # Sicherstellen, dass X_train und X_test Pandas-DataFrames mit gleichen Spaltennamen sind\n",
    "    if not isinstance(X_train, pd.DataFrame):\n",
    "        X_train = pd.DataFrame(X_train, columns=[f\"feature_{i}\" for i in range(X_train.shape[1])])\n",
    "    if not isinstance(X_test, pd.DataFrame):\n",
    "        X_test = pd.DataFrame(X_test, columns=X_train.columns)  # Konsistente Feature-Namen sicherstellen\n",
    "\n",
    "    # Modelle und Feature-Namen speichern in diesen Dateien\n",
    "    model_file = os.path.join(models_dir, 'lightgbm_model.pkl')\n",
    "    feature_names_file = os.path.join(models_dir, 'lightgbm_feature_names.pkl')\n",
    "    model = None\n",
    "\n",
    "    # Prüfe, ob das Modell schon existiert und geladen werden kann\n",
    "    if use_saved_models and os.path.exists(model_file):\n",
    "        try:\n",
    "            print(\"\\nGespeichertes LightGBM-Modell gefunden. Laden...\")\n",
    "            model = joblib.load(model_file)\n",
    "\n",
    "            # Lade auch Feature-Namen\n",
    "            if os.path.exists(feature_names_file):\n",
    "                with open(feature_names_file, 'rb') as f:\n",
    "                    saved_feature_names = joblib.load(f)\n",
    "\n",
    "                # Sicherstellen, dass die gelesenen Feature-Namen zu den jetzigen passen\n",
    "                if list(X_train.columns) != saved_feature_names:\n",
    "                    raise ValueError(\"Die Feature-Namen aus den gespeicherten Daten passen nicht zu X_train!\")\n",
    "        except Exception as e:\n",
    "            print(f\"Fehler beim Laden des gespeicherten Modells oder der Daten: {e}\")\n",
    "            model = None\n",
    "\n",
    "    # Modell trainieren, wenn keines geladen werden konnte\n",
    "    if model is None:\n",
    "        print(\"\\nKein Modell gefunden. Starte Training für LightGBM...\")\n",
    "        model = LGBMClassifier(random_state=42, n_estimators=100)\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # Modell und Feature-Namen speichern\n",
    "        try:\n",
    "            joblib.dump(model, model_file)\n",
    "            print(f\"LightGBM-Modell gespeichert als '{model_file}'\")\n",
    "\n",
    "            with open(feature_names_file, 'wb') as f:\n",
    "                joblib.dump(list(X_train.columns), f)\n",
    "                print(f\"Feature-Namen gespeichert als '{feature_names_file}'\")\n",
    "        except Exception as e:\n",
    "            print(f\"Fehler beim Speichern des Modells oder der Feature-Namen: {e}\")\n",
    "\n",
    "    # Vorhersagen und Wahrscheinlichkeiten berechnen\n",
    "    y_train_proba = model.predict_proba(X_train)[:, 1]  # Klassenwahrscheinlichkeiten\n",
    "    y_test_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    # Klassenschwellenwert\n",
    "    threshold = 0.5\n",
    "    y_train_pred = (y_train_proba > threshold).astype(int)\n",
    "    y_test_pred = (y_test_proba > threshold).astype(int)\n",
    "\n",
    "    # Zusätzliche Metriken berechnen\n",
    "    roc_auc = roc_auc_score(y_test, y_test_proba) if hasattr(model, \"predict_proba\") else None\n",
    "    results = {\n",
    "        \"model_name\": \"LightGBM\",\n",
    "        \"trained_model\": model,\n",
    "        \"train_accuracy\": accuracy_score(y_train, y_train_pred),\n",
    "        \"test_accuracy\": accuracy_score(y_test, y_test_pred),\n",
    "        \"roc_auc_score\": roc_auc,\n",
    "        \"f1_score\": f1_score(y_test, y_test_pred, zero_division=0),\n",
    "        \"precision\": precision_score(y_test, y_test_pred, zero_division=0),\n",
    "        \"recall\": recall_score(y_test, y_test_pred, zero_division=0),\n",
    "        \"y_test_proba\": y_test_proba\n",
    "    }\n",
    "\n",
    "    # Ergebnisse ausgeben\n",
    "    print(f\"\\nTrainings-Genauigkeit: {results['train_accuracy']:.4f}\")\n",
    "    print(f\"Test-Genauigkeit: {results['test_accuracy']:.4f}\")\n",
    "    print(f\"ROC-AUC: {results['roc_auc_score']:.4f}\")\n",
    "    print(f\"F1-Score: {results['f1_score']:.4f}\")\n",
    "    print(f\"Präzision: {results['precision']:.4f}\")\n",
    "    print(f\"Recall: {results['recall']:.4f}\")\n",
    "    print(\"=\" * 24)\n",
    "\n",
    "    return results\n",
    "\n",
    "def train_neural_network(X_train, X_test, y_train, y_test, use_saved_models, models_dir):\n",
    "    \"\"\"\n",
    "    Trainiert ein neuronales Netzwerk\n",
    "    oder lädt ein gespeichertes Modell, falls vorhanden.\n",
    "    \"\"\"\n",
    "    model_file = os.path.join(models_dir, 'nn_model.keras')\n",
    "\n",
    "    # Prüfen, ob ein gespeichertes Modell vorhanden ist\n",
    "    if use_saved_models and os.path.exists(model_file):\n",
    "        print(f\"\\nGespeichertes Keras-Modell gefunden. Laden...\")\n",
    "        model = load_model(model_file)\n",
    "\n",
    "        # Prüfen, ob Input-Dimensionen übereinstimmen\n",
    "        input_shape = model.input_shape[1]\n",
    "        if input_shape != X_train.shape[1]:\n",
    "            raise ValueError(f\"Das gespeicherte Modell erwartet Eingabedimension {input_shape}, \"\n",
    "                             f\"aber die aktuellen Daten haben {X_train.shape[1]} Features.\")\n",
    "    else:\n",
    "        print(f\"\\nTraining für ein neuronales Netzwerk startet...\")\n",
    "        feature_count = X_train.shape[1]\n",
    "\n",
    "        # Schritt 3: Modell erstellen\n",
    "        model = Sequential([\n",
    "            layers.Input(shape=(X_train.shape[1],)),\n",
    "            layers.Dense(64, activation='relu'),\n",
    "            layers.Dropout(0.3),\n",
    "            layers.Dense(32, activation='relu'),\n",
    "            layers.Dense(1, activation='sigmoid')\n",
    "        ])\n",
    "\n",
    "        # Kompilierung des Modells\n",
    "        model.compile(optimizer='Adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "        try:\n",
    "            # Modell trainieren\n",
    "            model.fit(X_train, y_train, epochs=10, batch_size=64, validation_split=0.2, verbose=0)\n",
    "        except Exception as e:\n",
    "            print(f\"Fehler beim Training des neuronalen Netzwerks: {e}\")\n",
    "            return None\n",
    "\n",
    "        # Modell speichern\n",
    "        model.save(model_file)\n",
    "        print(f\"Keras-Modell gespeichert als '{model_file}'\")\n",
    "\n",
    "    # Wahrscheinlichkeiten berechnen\n",
    "    y_train_proba = model.predict(X_train).ravel()\n",
    "    y_test_proba = model.predict(X_test).ravel()\n",
    "\n",
    "    # Wahrscheinlichkeitsverteilung prüfen\n",
    "    plt.hist(y_test_proba, bins=30, alpha=0.8, color='blue')\n",
    "    plt.title(\"Histogramm der vorhergesagten Wahrscheinlichkeiten\")\n",
    "    plt.xlabel(\"Wahrscheinlichkeit\")\n",
    "    plt.ylabel(\"Häufigkeit\")\n",
    "    plt.show()\n",
    "\n",
    "    # Dynamische Schwellenwertanpassung (z. B. anhand Precision-Recall)\n",
    "    precision, recall, thresholds = precision_recall_curve(y_test, y_test_proba)\n",
    "    # Wähle Schwellenwert, der Precision > 0.6 garantiert\n",
    "    threshold = thresholds[np.argmax(precision > 0.6)] if np.any(precision > 0.6) else 0.5\n",
    "    print(f\"Automatisch angepasster Schwellenwert: {threshold:.2f}\")\n",
    "\n",
    "    y_train_pred = (y_train_proba > threshold).astype(int)\n",
    "    y_test_pred = (y_test_proba > threshold).astype(int)\n",
    "\n",
    "    # Bewertung des Modells\n",
    "    result = {\n",
    "        \"model_name\": \"Neural Network\",\n",
    "        \"trained_model\": model,\n",
    "        \"train_accuracy\": accuracy_score(y_train, y_train_pred),\n",
    "        \"test_accuracy\": accuracy_score(y_test, y_test_pred),\n",
    "        \"roc_auc_score\": roc_auc_score(y_test, y_test_proba),\n",
    "        \"y_test_proba\": y_test_proba,\n",
    "        \"f1_score\": f1_score(y_test, y_test_pred, zero_division=1),\n",
    "        \"precision\": precision_score(y_test, y_test_pred, zero_division=1),\n",
    "        \"recall\": recall_score(y_test, y_test_pred, zero_division=1),\n",
    "    }\n",
    "\n",
    "    # Ausgabe überprüfen: Prüfung auf leere Ergebnisse\n",
    "    critical_count = np.sum(y_test_pred)\n",
    "    if critical_count == 0:\n",
    "        print(\"\\nWARNUNG: Keine kritischen Mitarbeiter gefunden. Prüfen Sie Daten und Modell!\")\n",
    "    else:\n",
    "        print(f\"Kritische Mitarbeiter identifiziert: {critical_count}\")\n",
    "\n",
    "    # Bewertungsergebnisse ausgeben\n",
    "    print(f\"\\nTrainings-Genauigkeit: {result['train_accuracy']:.4f}\")\n",
    "    print(f\"Test-Genauigkeit: {result['test_accuracy']:.4f}\")\n",
    "    print(f\"ROC-AUC: {result['roc_auc_score']:.4f}\")\n",
    "    print(f\"F1-Score: {result['f1_score']:.4f}\")\n",
    "    print(f\"Präzision: {result['precision']:.4f}\")\n",
    "    print(f\"Recall: {result['recall']:.4f}\")\n",
    "    print(\"=\" * 24)\n",
    "\n",
    "    return result\n",
    "\n",
    "def evaluate_models(models, X_test, y_test, plot_dir=\"plots\"):\n",
    "    \"\"\"\n",
    "    Bewertet ML-Modelle und erstellt Verwirrungsmatrix-Subplots.\n",
    "    \"\"\"\n",
    "    evaluation_results = {\n",
    "        \"Model\": [],\n",
    "        \"ROC-AUC\": [],\n",
    "        \"F1-Score\": [],\n",
    "        \"Precision\": [],\n",
    "        \"Recall\": [],\n",
    "        \"Accuracy\": [],\n",
    "        \"MCC\": [],\n",
    "        \"Log-Loss\": [],\n",
    "        \"TP\": [],\n",
    "        \"TN\": [],\n",
    "        \"FP\": [],\n",
    "        \"FN\": []\n",
    "    }\n",
    "\n",
    "    # Modelle evaluieren\n",
    "    for name, model in models.items():\n",
    "        if model is None:\n",
    "            print(f\"Warnung: Modell '{name}' ist None und wird übersprungen.\")\n",
    "            continue\n",
    "        try:\n",
    "            # Vorhersagen erstellen\n",
    "            if hasattr(model, \"predict_proba\") and callable(getattr(model, \"predict_proba\")):\n",
    "                y_proba = model.predict_proba(X_test)[:, 1]\n",
    "            else:\n",
    "                # Für Modelle ohne `predict_proba`:\n",
    "                y_proba = model.predict(X_test)\n",
    "                if len(y_proba.shape) > 1:\n",
    "                    y_proba = y_proba[:, 0]\n",
    "\n",
    "            # Binäre Klassifikation basierend auf 0.5-Schwellenwert\n",
    "            y_pred = (y_proba >= 0.5).astype(int)\n",
    "\n",
    "            # Metriken berechnen\n",
    "            roc_auc = roc_auc_score(y_test, y_proba)\n",
    "            f1 = f1_score(y_test, y_pred)\n",
    "            precision = precision_score(y_test, y_pred)\n",
    "            recall = recall_score(y_test, y_pred)\n",
    "            accuracy = accuracy_score(y_test, y_pred)\n",
    "            mcc = matthews_corrcoef(y_test, y_pred)\n",
    "            log_loss_val = log_loss(y_test, y_proba)\n",
    "\n",
    "            # Verwirrungsmatrix berechnen -> TN, FP, FN, TP\n",
    "            tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "\n",
    "            # Ergebnisse speichern\n",
    "            evaluation_results[\"Model\"].append(name)\n",
    "            evaluation_results[\"ROC-AUC\"].append(roc_auc)\n",
    "            evaluation_results[\"F1-Score\"].append(f1)\n",
    "            evaluation_results[\"Precision\"].append(precision)\n",
    "            evaluation_results[\"Recall\"].append(recall)\n",
    "            evaluation_results[\"Accuracy\"].append(accuracy)\n",
    "            evaluation_results[\"MCC\"].append(mcc)\n",
    "            evaluation_results[\"Log-Loss\"].append(log_loss_val)\n",
    "            evaluation_results[\"TP\"].append(tp)\n",
    "            evaluation_results[\"TN\"].append(tn)\n",
    "            evaluation_results[\"FP\"].append(fp)\n",
    "            evaluation_results[\"FN\"].append(fn)\n",
    "\n",
    "            print(f\"Modell '{name}' bewertet:\")\n",
    "            print(f\"  ROC-AUC: {roc_auc:.4f}, F1-Score: {f1:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}\")\n",
    "            print(f\"  Accuracy: {accuracy:.4f}, MCC: {mcc:.4f}, Log-Loss: {log_loss_val:.4f}\")\n",
    "            print(f\"  Confusion Matrix - TP: {tp}, TN: {tn}, FP: {fp}, FN: {fn}\\n\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Fehler beim Evaluieren des Modells '{name}': {e}\")\n",
    "\n",
    "    results_df = pd.DataFrame(evaluation_results)\n",
    "\n",
    "    try:\n",
    "        model_names = results_df[\"Model\"]\n",
    "        tp = results_df[\"TP\"]\n",
    "        tn = results_df[\"TN\"]\n",
    "        fp = results_df[\"FP\"]\n",
    "        fn = results_df[\"FN\"]\n",
    "\n",
    "        num_models = len(model_names)\n",
    "        fig, axes = plt.subplots(num_models, 1, figsize=(8, 6 * num_models), squeeze=False)\n",
    "\n",
    "        for i, ax in enumerate(axes.flatten()):\n",
    "            matrix = np.array([[tn[i], fp[i]], [fn[i], tp[i]]])\n",
    "            ax.imshow(matrix, interpolation=\"nearest\", cmap=\"Blues\")  # Direkte Farbschema-Verwendung\n",
    "\n",
    "            ax.set_title(f\"Confusion Matrix - {model_names[i]}\")\n",
    "            ax.set_xticks([0, 1])\n",
    "            ax.set_xticklabels([\"Negative\", \"Positive\"])\n",
    "            ax.set_yticks([0, 1])\n",
    "            ax.set_yticklabels([\"Negative\", \"Positive\"])\n",
    "\n",
    "            # Hinzufügen von Text innerhalb jeder Matrix-Zelle\n",
    "            for j in range(2):\n",
    "                for k in range(2):\n",
    "                    ax.text(k, j, format(matrix[j, k], \"d\"),\n",
    "                            horizontalalignment=\"center\",\n",
    "                            color=\"white\" if matrix[j, k] > matrix.max() / 2 else \"black\")  # Dynamische Farbe\n",
    "\n",
    "        plt.tight_layout()\n",
    "        os.makedirs(plot_dir, exist_ok=True)\n",
    "        plot_path = os.path.join(plot_dir, \"combined_confusion_matrices.png\")\n",
    "        plt.savefig(plot_path, dpi=300)\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Fehler bei der Erstellung des Verwirrungsmatrix-Plots: {e}\")\n",
    "\n",
    "    return results_df\n",
    "\n",
    "def plot_horizontal_comparison(\n",
    "        model_names,\n",
    "        train_accuracies,\n",
    "        test_accuracies,\n",
    "        roc_auc_scores,\n",
    "        plot_dir,\n",
    "        f1_scores=None,\n",
    "        precisions=None,\n",
    "        recalls=None,\n",
    "        sort_by=\"Test Accuracy\",\n",
    "        include_train=True,\n",
    "        bar_colors=(\"skyblue\", \"steelblue\", \"lightgreen\", \"orange\", \"purple\", \"brown\"),\n",
    "        xlabel=\"Performance\",\n",
    "        legend_labels=(\"Train Accuracy\", \"Test Accuracy\", \"ROC AUC\", \"F1 Score\", \"Precision\", \"Recall\")\n",
    "):\n",
    "    \"\"\"\n",
    "    Erstellt ein horizontales Balkendiagramm, um Modellleistung zu vergleichen.\n",
    "\n",
    "    Parameter:\n",
    "    -----------\n",
    "    model_names : list\n",
    "        Liste der Modellnamen.\n",
    "    train_accuracies : list\n",
    "        Liste der Trainingsgenauigkeiten (optional einblendbar).\n",
    "    test_accuracies : list\n",
    "        Liste der Testgenauigkeiten.\n",
    "    roc_auc_scores : list\n",
    "        Liste der ROC-AUC-Werte.\n",
    "    training_times : list\n",
    "        Liste der Trainingszeiten in Sekunden (werden als Annotation angezeigt).\n",
    "    f1_scores : list, optional\n",
    "        Liste der F1-Scores, falls vorhanden.\n",
    "    precisions : list, optional\n",
    "        Liste der Präzisionswerte, falls vorhanden.\n",
    "    recalls : list, optional\n",
    "        Liste der Recall-Werte, falls vorhanden.\n",
    "    plot_dir : str\n",
    "        Verzeichnis zum Speichern des Diagramms.\n",
    "    sort_by : str\n",
    "        Metrik, nach der die Modelle sortiert werden (`\"Train Accuracy\"`, `\"Test Accuracy\"`, `\"ROC AUC\"`,\n",
    "        `\"Training Time (s)\"`, `\"F1 Score\"`, `\"Precision\"`, `\"Recall\"`).\n",
    "    include_train : bool\n",
    "        Ob `Train Accuracy` dargestellt werden soll (optional).\n",
    "    bar_colors : tuple\n",
    "        Farben der Balken für Train, Test, ROC AUC, F1 Score, Precision und Recall.\n",
    "    xlabel : str\n",
    "        Beschriftung der x-Achse (dynamische Sprache möglich).\n",
    "    legend_labels : tuple\n",
    "        Legendenbeschriftung für Train, Test, ROC AUC, F1 Score, Precision und Recall.\n",
    "    \"\"\"\n",
    "    # Prüfen auf Verzeichnis\n",
    "    if not os.path.exists(plot_dir):\n",
    "        os.makedirs(plot_dir)\n",
    "\n",
    "    # Daten vorbereiten (neue Metriken optional hinzufügen)\n",
    "    data = pd.DataFrame({\n",
    "        \"Model\": model_names,\n",
    "        \"Train Accuracy\": train_accuracies,\n",
    "        \"Test Accuracy\": test_accuracies,\n",
    "        \"ROC AUC\": roc_auc_scores,\n",
    "    })\n",
    "\n",
    "    # Optional: Zusätzliche Spalten hinzufügen, wenn Daten verfügbar\n",
    "    if f1_scores:\n",
    "        data[\"F1 Score\"] = f1_scores\n",
    "    if precisions:\n",
    "        data[\"Precision\"] = precisions\n",
    "    if recalls:\n",
    "        data[\"Recall\"] = recalls\n",
    "\n",
    "    # Dynamische Sortierung der Modelle\n",
    "    if sort_by in data.columns:\n",
    "        data = data.sort_values(by=sort_by, ascending=False)  # Absteigend sortieren\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid sort_by parameter: {sort_by}. Must be one of {', '.join(data.columns)}\")\n",
    "\n",
    "    # Diagrammerstellung\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    bar_width = 0.10  # Breite der Balken für verschiedene Metriken\n",
    "    y_pos = np.arange(len(data))  # Y-Positionen für die Balken\n",
    "\n",
    "    # Balkendiagramme nach Metrik\n",
    "    if include_train:\n",
    "        ax.barh(y_pos - 2 * bar_width, data[\"Train Accuracy\"], bar_width, label=legend_labels[0], color=bar_colors[0])\n",
    "    ax.barh(y_pos - bar_width, data[\"Test Accuracy\"], bar_width, label=legend_labels[1], color=bar_colors[1])\n",
    "    ax.barh(y_pos, data[\"ROC AUC\"], bar_width, label=legend_labels[2], color=bar_colors[2])\n",
    "\n",
    "    # Zusätzliche Metriken optional einfügen\n",
    "    if \"F1 Score\" in data.columns:\n",
    "        ax.barh(y_pos + bar_width, data[\"F1 Score\"], bar_width, label=legend_labels[3], color=bar_colors[3])\n",
    "    if \"Precision\" in data.columns:\n",
    "        ax.barh(y_pos + 2 * bar_width, data[\"Precision\"], bar_width, label=legend_labels[4], color=bar_colors[4])\n",
    "    if \"Recall\" in data.columns:\n",
    "        ax.barh(y_pos + 3 * bar_width, data[\"Recall\"], bar_width, label=legend_labels[5], color=bar_colors[5])\n",
    "\n",
    "\n",
    "    # Achsenbeschriftung, Titel und Legende\n",
    "    ax.set_yticks(y_pos)\n",
    "    ax.set_yticklabels(data[\"Model\"])\n",
    "    ax.set_title(\"Modellvergleich\")\n",
    "    ax.set_xlabel(xlabel)\n",
    "    ax.legend()\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Diagramm speichern\n",
    "    output_path = os.path.join(plot_dir, \"model_performance_comparison.png\")\n",
    "    plt.savefig(output_path)\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"Diagramm gespeichert unter: {output_path}\")\n",
    "\n",
    "def plot_combined_roc_curves(models, X_test, y_test, plot_dir, nn_model=None, y_proba_nn=None ):\n",
    "    \"\"\"\n",
    "    Plot für ROC-Kurven aller Modelle erstellen, einschließlich des neuronalen Netzwerks.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        os.makedirs(plot_dir, exist_ok=True)\n",
    "\n",
    "        plt.figure(figsize=(16, 10))\n",
    "        valid_models = False  # Prüfen, ob zumindest ein Modell erfolgreich\n",
    "\n",
    "        # Plot der traditionellen Modelle\n",
    "        for name, model in models.items():\n",
    "            if hasattr(model, \"predict_proba\"):\n",
    "                # Wahrscheinlichkeiten abrufen\n",
    "                y_proba = model.predict_proba(X_test)[:, 1]\n",
    "                fpr, tpr, _ = roc_curve(y_test, y_proba)\n",
    "                roc_auc = auc(fpr, tpr)\n",
    "                plt.plot(fpr, tpr, label=f\"ROC-{name} (AUC={roc_auc:.2f})\")\n",
    "                valid_models = True\n",
    "\n",
    "        # Plot für das neuronale Netzwerk\n",
    "        if nn_model and y_proba_nn is not None:\n",
    "            fpr, tpr, _ = roc_curve(y_test, y_proba_nn)\n",
    "            roc_auc = auc(fpr, tpr)\n",
    "            plt.plot(fpr, tpr, label=f\"ROC-NeuralNet (AUC={roc_auc:.2f})\", linestyle=\"--\", color=\"purple\")\n",
    "            valid_models = True\n",
    "\n",
    "        # Zufallsklassifikationsgrenze\n",
    "        if valid_models:\n",
    "            plt.plot([0, 1], [0, 1], \"k--\", label=\"Random Baseline\")\n",
    "            plt.legend(loc=\"lower right\")\n",
    "            plt.title(\"ROC-Kurven\")\n",
    "            plt.xlabel(\"False Positive Rate\")\n",
    "            plt.ylabel(\"True Positive Rate\")\n",
    "            plt.grid(True)\n",
    "            plt.savefig(os.path.join(plot_dir, \"all_roc_curves.png\"))\n",
    "            plt.show()\n",
    "        else:\n",
    "            print(\"Keine validen Modelle für ROC-Kurven gefunden.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Fehler bei der Erstellung des ROC-Plots: {e}\")\n",
    "\n",
    "def test_under_over_fit(model_names, train_accuracies, test_accuracies, overfit_threshold=0.1, underfit_threshold=0.6):\n",
    "    \"\"\"\n",
    "    Identifiziert Overfitting und Underfitting für gegebene Modelle basierend auf Genauigkeitswerten.\n",
    "\n",
    "    Args:\n",
    "        model_names (list): Liste der Modellnamen.\n",
    "        train_accuracies (list): Trainingsgenauigkeiten der Modelle.\n",
    "        test_accuracies (list): Testgenauigkeiten der Modelle.\n",
    "        overfit_threshold (float): Schwellenwert für Overfitting (Train-Test-Differenz).\n",
    "        underfit_threshold (float): Schwellenwert für Underfitting (Train- und Testgenauigkeit < Threshold).\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame mit den Ergebnissen (\"Model\", \"Overfit\", \"Underfit\").\n",
    "    \"\"\"\n",
    "    results = {\"Model\": [], \"Overfit\": [], \"Underfit\": []}\n",
    "\n",
    "    for name, train, test in zip(model_names, train_accuracies, test_accuracies):\n",
    "        overfit = (train - test) > overfit_threshold\n",
    "        underfit = (train < underfit_threshold) and (test < underfit_threshold)\n",
    "\n",
    "        results[\"Model\"].append(name)\n",
    "        results[\"Overfit\"].append(overfit)\n",
    "        results[\"Underfit\"].append(underfit)\n",
    "\n",
    "        # Ausgabe nur in Variablen speichern (optional für Debugging)\n",
    "        log_message = (\n",
    "            f\"Model: {name}\\n\"\n",
    "            f\"  Overfit: {overfit} (Train-Test Difference: {train - test:.3f})\\n\"\n",
    "            f\"  Underfit: {underfit} (Train Accuracy: {train:.3f}, Test Accuracy: {test:.3f})\\n\"\n",
    "        )\n",
    "\n",
    "    # Ergebnisse in DataFrame umwandeln\n",
    "    result_df = pd.DataFrame(results)\n",
    "\n",
    "    return result_df\n",
    "\n",
    "def plot_under_over_fit(model_names, train_accuracies, test_accuracies, plot_dir):\n",
    "    \"\"\"\n",
    "    Visualisiert Trainings- und Testabweichungen, um Overfitting oder Underfitting zu bewerten.\n",
    "\n",
    "    Args:\n",
    "        model_names (list): Liste von Modellnamen.\n",
    "        train_accuracies (list): Trainingsgenauigkeiten.\n",
    "        test_accuracies (list): Testgenauigkeiten.\n",
    "        plot_dir (str): Speicherort des generierten Plots.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        differences = np.array(train_accuracies) - np.array(test_accuracies)\n",
    "        colors = [\"green\" if -0.1 <= d <= 0.1 else \"red\" if d > 0.1 else \"orange\" for d in differences]\n",
    "\n",
    "        data = pd.DataFrame({\n",
    "            \"Model\": model_names,\n",
    "            \"Train Accuracy\": train_accuracies,\n",
    "            \"Test Accuracy\": test_accuracies,\n",
    "            \"Train-Test Difference\": differences\n",
    "        }).sort_values(by=\"Train-Test Difference\", ascending=False)\n",
    "\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        bars = plt.barh(data[\"Model\"], data[\"Train-Test Difference\"], color=colors)\n",
    "        plt.axvline(x=0, color=\"black\", linestyle=\"--\", linewidth=1, label=\"Balanced Fit\")\n",
    "        plt.xlabel(\"Train-Test Accuracy Difference\", fontsize=12)\n",
    "        plt.ylabel(\"Models\", fontsize=12)\n",
    "        plt.title(\"Model Analysis: Underfitting vs Overfitting\", fontsize=14)\n",
    "\n",
    "        for bar, diff, train, test in zip(bars, data[\"Train-Test Difference\"], data[\"Train Accuracy\"],\n",
    "                                          data[\"Test Accuracy\"]):\n",
    "            plt.text(bar.get_width(), bar.get_y() + bar.get_height() / 2,\n",
    "                     f\"Diff: {diff:.2f}\\nTrain: {train:.2f}\\nTest: {test:.2f}\",\n",
    "                     va='center', ha='left', fontsize=9)\n",
    "\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.gca().invert_yaxis()\n",
    "\n",
    "        os.makedirs(plot_dir, exist_ok=True)\n",
    "        plot_path = os.path.join(plot_dir, \"under_over_fit_analysis.png\")\n",
    "        plt.savefig(plot_path)\n",
    "        plt.show()\n",
    "\n",
    "        print(f\"Under-/Overfitting-Analyse gespeichert unter: {plot_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Fehler in der Funktion 'plot_under_over_fit': {e}\")\n",
    "\n",
    "def get_best_model(models, evaluation_results, fit_results, primary_metric=\"ROC-AUC\", tie_breaker=\"F1-Score\"):\n",
    "    \"\"\"\n",
    "    Wählt das beste Modell basierend auf einer Hauptmetrik aus, schließt aber Modelle mit Over-/Underfitting aus.\n",
    "\n",
    "    Args:\n",
    "        models (dict): Dictionary der trainierten Modelle.\n",
    "        evaluation_results (pd.DataFrame): DataFrame mit den Metriken aller Modelle.\n",
    "        fit_results (pd.DataFrame): DataFrame mit den FIT-Analyseergebnissen (\"Overfit\", \"Underfit\").\n",
    "        primary_metric (str): Hauptmetrik, nach der das beste Modell ausgewählt wird (z.B.: \"ROC-AUC\").\n",
    "        tie_breaker (str): Sekundärmetrik, die bei einem Gleichstand verwendet wird.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Bestes Modell und der Name des Modells (als String).\n",
    "    \"\"\"\n",
    "    # Kombiniere die FIT-Ergebnisse mit den Modell-Metriken\n",
    "    if \"Model\" not in fit_results.columns or \"Overfit\" not in fit_results.columns or \"Underfit\" not in fit_results.columns:\n",
    "        raise ValueError(\"Die FIT-Ergebnisse müssen die Spalten 'Model', 'Overfit' und 'Underfit' enthalten.\")\n",
    "\n",
    "    combined_results = pd.merge(evaluation_results, fit_results, on=\"Model\", how=\"inner\")\n",
    "\n",
    "    # Filtere alle Modelle aus, die Overfitting oder Underfitting aufweisen\n",
    "    valid_models = combined_results[(combined_results[\"Overfit\"] == False) & (combined_results[\"Underfit\"] == False)]\n",
    "\n",
    "    if valid_models.empty:\n",
    "        raise ValueError(\"Keine Modelle erfüllen die Kriterien: Kein Overfit und kein Underfit.\")\n",
    "\n",
    "    # Modell basierend auf der Hauptmetrik auswählen\n",
    "    if primary_metric not in valid_models.columns:\n",
    "        raise ValueError(\n",
    "            f\"Die Metrik '{primary_metric}' ist nicht in den Ergebnissen vorhanden. Verfügbare Metriken: {valid_models.columns.tolist()}\")\n",
    "\n",
    "    best_model_data = valid_models.loc[valid_models[primary_metric].idxmax()]\n",
    "    top_candidates = valid_models[valid_models[primary_metric] == best_model_data[primary_metric]]\n",
    "\n",
    "    # Bei Gleichstand verwende die Sekundärmetrik\n",
    "    if len(top_candidates) > 1 and tie_breaker in valid_models.columns:\n",
    "        top_candidates = top_candidates.sort_values(by=tie_breaker, ascending=False)\n",
    "        best_model_data = top_candidates.iloc[0]\n",
    "\n",
    "    # Namen und Modell extrahieren\n",
    "    best_model_name = best_model_data[\"Model\"]\n",
    "    best_model = models[best_model_name]\n",
    "\n",
    "    print(\n",
    "        f\"Das beste Modell basierend auf '{primary_metric}' ist '{best_model_name}' mit einem Wert von {best_model_data[primary_metric]:.4f}.\")\n",
    "    if len(top_candidates) > 1:\n",
    "        print(f\"  (Gleichstand gelöst durch '{tie_breaker}'.)\")\n",
    "    return best_model, best_model_name\n",
    "\n",
    "def get_critical_employees(model, X_transformed, df, scaler_file=\"Models/scaler.pkl\",\n",
    "                           feature_names_file=\"Models/lightgbm_feature_names.pkl\", pca=None, threshold=0.0):\n",
    "    \"\"\"\n",
    "    Identifiziert kritische Mitarbeiter mit einer Fluktuationswahrscheinlichkeit höher\n",
    "    als der angegebenen Schwelle unter Verwendung eines gespeicherten Scalers.\n",
    "    Optional wird PCA angewendet. Funktioniert für LightGBM-Modelle und andere sklearn-ähnliche Modelle.\n",
    "\n",
    "    Args:\n",
    "        model: Das trainierte Modell für die Vorhersage.\n",
    "        X_transformed (np.ndarray): Die preprocessierten Features (unskaliert).\n",
    "        df (pd.DataFrame): Der DataFrame mit zusätzlichen Informationen (z. B. Mitarbeiterdaten).\n",
    "        scaler_file (str): Dateiname des gespeicherten Scalers.\n",
    "        feature_names_file (str): Pfad zu den gespeicherten Feature-Namen (für LightGBM).\n",
    "        pca (PCA, optional): Ein optionales PCA-Objekt für Dimensionenreduktion.\n",
    "        threshold (float): Der Schwellenwert für die Fluktuationswahrscheinlichkeit.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[pd.DataFrame, pd.DataFrame]: Kritische Mitarbeiter und die Top 15 Mitarbeiter nach Wahrscheinlichkeit.\n",
    "    \"\"\"\n",
    "\n",
    "    print(f\"Shape von X_transformed: {X_transformed.shape}\")\n",
    "    print(f\"Shape von df: {df.shape}\")\n",
    "\n",
    "    # Dynamisch letzten Monat und letztes Jahr aus dem Datensatz bestimmen\n",
    "    max_year = df[\"Jahr\"].max()  # Höchstes Jahr im Datensatz\n",
    "    max_month_in_max_year = df[df[\"Jahr\"] == max_year][\"Monat\"].max()  # Höchster Monat im höchsten Jahr\n",
    "    print(f\"Letztes Jahr im Datensatz: {max_year}, letzter Monat im Jahr: {max_month_in_max_year}\")\n",
    "\n",
    "    # ** Scaler laden und anwenden **\n",
    "    print(f\"Lade Scaler aus Datei '{scaler_file}'...\")\n",
    "    try:\n",
    "        scaler = joblib.load(scaler_file)  # Scaler laden\n",
    "        X_transformed = scaler.transform(X_transformed)  # Daten skalieren\n",
    "        print(\"Testdaten erfolgreich skaliert.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Fehler beim Laden oder Anwenden des Scalers: {e}\")\n",
    "        return None, None\n",
    "\n",
    "    # ** Feature-Namen laden und auf die Eingabedaten anwenden (nur für LightGBM notwendig) **\n",
    "    if isinstance(model, lgb.LGBMClassifier) and feature_names_file:\n",
    "        try:\n",
    "            with open(feature_names_file, 'rb') as f:\n",
    "                feature_names = joblib.load(f)\n",
    "            print(f\"Feature-Namen erfolgreich geladen: {feature_names}\")\n",
    "\n",
    "            # Konvertiere X_transformed in einen DataFrame mit den korrekten Spaltennamen\n",
    "            X_transformed = pd.DataFrame(X_transformed, columns=feature_names)\n",
    "        except Exception as e:\n",
    "            print(f\"Fehler beim Laden oder Anwenden der Feature-Namen: {e}\")\n",
    "            return None, None\n",
    "\n",
    "    # ** PCA nur dann laden, wenn das Modell Logistic Regression ist **\n",
    "    if pca is None and isinstance(model, LogisticRegression):\n",
    "        pca_path = \"Models/Back/pca_model.pkl\"\n",
    "        if os.path.exists(pca_path):\n",
    "            try:\n",
    "                print(f\"Lade gespeichertes PCA-Modell aus '{pca_path}'...\")\n",
    "                pca = joblib.load(pca_path)\n",
    "                X_transformed = pca.transform(X_transformed)\n",
    "                print(f\"PCA-Transformation erfolgreich angewendet. Shape: {X_transformed.shape}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Fehler beim Laden oder Anwenden von PCA: {e}\")\n",
    "                return None, None\n",
    "        else:\n",
    "            print(f\"[WARNUNG] Kein gespeichertes PCA-Modell unter '{pca_path}' gefunden. Fortfahren ohne PCA.\")\n",
    "\n",
    "    # ** Berechnung der Vorhersagen **\n",
    "    print(\"Berechnung der Vorhersagen...\")\n",
    "    try:\n",
    "        if hasattr(model, \"predict_proba\"):\n",
    "            # Für sklearn-ähnliche Modelle, einschließlich LightGBM\n",
    "            y_proba = model.predict_proba(X_transformed)[:, 1]\n",
    "        elif hasattr(model, \"predict\"):\n",
    "            # Für Modelle wie Keras (z. B. ohne predict_proba)\n",
    "            y_proba = model.predict(X_transformed).flatten()\n",
    "        else:\n",
    "            raise AttributeError(f\"Modelltyp '{type(model)}' wird nicht unterstützt.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Fehler bei den Vorhersagen: {e}\")\n",
    "        return None, None\n",
    "\n",
    "    # ** Verknüpfung der Wahrscheinlichkeiten mit den Daten **\n",
    "    try:\n",
    "        # Fluktuationswahrscheinlichkeit in Prozent umrechnen\n",
    "        df[\"Fluktuationswahrscheinlichkeit\"] = y_proba * 100\n",
    "    except ValueError as e:\n",
    "        print(f\"Fehler beim Hinzufügen der Fluktuationswahrscheinlichkeit: {e}\")\n",
    "        return None, None\n",
    "\n",
    "    # ** Kritische Mitarbeiter identifizieren **\n",
    "    try:\n",
    "        # Filterung: Nur Mitarbeiter mit Fluktuation aktiv (0) und aus dem letzten Monat/Jahr\n",
    "        df = df[\n",
    "            (df[\"Fluktuation\"] == 0) &\n",
    "            (df[\"Monat\"] == max_month_in_max_year) &\n",
    "            (df[\"Jahr\"] == max_year)\n",
    "            ]\n",
    "        print(f\"Shape nach Filtern nach Fluktuation, Monat und Jahr: {df.shape}\")\n",
    "\n",
    "        critical_employees = df[df[\"Fluktuationswahrscheinlichkeit\"] > threshold]\n",
    "        critical_employees = critical_employees.sort_values(\n",
    "            by=\"Fluktuationswahrscheinlichkeit\", ascending=False\n",
    "        )\n",
    "\n",
    "        # Dubletten anhand von Mitarbeiter_ID entfernen\n",
    "        if \"Mitarbeiter_ID\" in critical_employees.columns:\n",
    "            duplicate_count = critical_employees.duplicated(subset=[\"Mitarbeiter_ID\"]).sum()\n",
    "            if duplicate_count > 0:\n",
    "                print(f\"Warnung: {duplicate_count} doppelte Einträge basierend auf 'Mitarbeiter_ID' wurden entfernt.\")\n",
    "                critical_employees = critical_employees.drop_duplicates(subset=[\"Mitarbeiter_ID\"])\n",
    "\n",
    "        # Top 15 kritische Mitarbeiter\n",
    "        top_15 = critical_employees.head(15)\n",
    "    except Exception as e:\n",
    "        print(f\"Fehler bei der Sortierung der kritischen Mitarbeiter: {e}\")\n",
    "        return None, None\n",
    "\n",
    "    # ** Rückgabe der Ergebnisse **\n",
    "    print(\"Kritische Mitarbeiter erfolgreich identifiziert.\")\n",
    "    return critical_employees, top_15\n",
    "\n",
    "def get_critical_employees_all_models(models, X_transformed, df, scaler_file=\"Models/scaler.pkl\",\n",
    "                                      feature_names_file=\"Models/lightgbm_feature_names.pkl\", pca=None, threshold=0.0):\n",
    "    \"\"\"\n",
    "    Identifiziert kritische Mitarbeiter und sammelt Ergebnisse für mehrere Modelle,\n",
    "    unter Berücksichtigung eines gespeicherten Scalers und optionaler PCA.\n",
    "    Unterstützt LightGBM mit gespeicherten Feature-Namen.\n",
    "\n",
    "    Args:\n",
    "        models (dict): Ein Dictionary der Modelle, wobei die Keys die Modellnamen sind.\n",
    "        X_transformed (np.ndarray): Die preprocessierten Features (unskaliert).\n",
    "        df (pd.DataFrame): Der DataFrame mit Mitarbeiterinformationen.\n",
    "        scaler_file (str): Dateiname des gespeicherten Scalers.\n",
    "        feature_names_file (str): Pfad zu den gespeicherten Feature-Namen (für LightGBM).\n",
    "        pca (PCA, optional): Ein optionales PCA-Objekt für Dimensionenreduktion.\n",
    "        threshold (float): Der Schwellenwert für die Fluktuationswahrscheinlichkeit.\n",
    "\n",
    "    Returns:\n",
    "        dict: Ein Dictionary mit Modellnamen als Keys und Tuple-Resultaten als Values.\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "\n",
    "    print(f\"\\nShape von X_transformed: {X_transformed.shape}\")\n",
    "    print(f\"Shape von df: {df.shape}\")\n",
    "\n",
    "    # Dynamisch letzten Monat und Jahr aus dem DataFrame bestimmen\n",
    "    max_year = df[\"Jahr\"].max()\n",
    "    max_month_in_max_year = df[df[\"Jahr\"] == max_year][\"Monat\"].max()\n",
    "    print(f\"Letztes Jahr im Datensatz: {max_year}, letzter Monat im Jahr: {max_month_in_max_year}\")\n",
    "\n",
    "    # ** Scaler laden und Testdaten skalieren **\n",
    "    print(f\"Lade Scaler aus '{scaler_file}'...\")\n",
    "    try:\n",
    "        scaler = joblib.load(scaler_file)\n",
    "        X_transformed = scaler.transform(X_transformed)\n",
    "        print(\"Testdaten erfolgreich skaliert.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Fehler beim Laden oder Anwenden des Scalers: {e}\")\n",
    "        return None\n",
    "\n",
    "    # ** Synchronisierung zwischen X_transformed und df **\n",
    "    if len(X_transformed) != len(df):\n",
    "        print(\"WARNUNG: Dimensionen von X_transformed und df stimmen nicht überein. Synchronisiere Daten...\")\n",
    "        df = df.iloc[:len(X_transformed)].reset_index(drop=True).copy()\n",
    "\n",
    "    # ** Iteration über alle Modelle **\n",
    "    for model_name, model in models.items():\n",
    "        print(f\"\\nBearbeite Modell: {model_name}\")\n",
    "        try:\n",
    "            # Zurücksetzen der transformierten Features für das Modell\n",
    "            X_model_transformed = X_transformed.copy()\n",
    "\n",
    "            # ** Spezialbehandlung für PCA oder LightGBM **\n",
    "            if isinstance(model, LogisticRegression):\n",
    "                if pca is not None:\n",
    "                    try:\n",
    "                        print(f\"PCA wird für das Modell '{model_name}' angewendet...\")\n",
    "                        X_model_transformed = pca.transform(X_model_transformed)\n",
    "                        print(f\"PCA-Transformation erfolgreich abgeschlossen. Shape: {X_model_transformed.shape}\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"Fehler bei PCA für Modell '{model_name}': {e}. Fortfahren ohne PCA.\")\n",
    "\n",
    "            elif isinstance(model, lgb.LGBMClassifier):\n",
    "                # Für LightGBM spezielle Feature-Namen anwenden\n",
    "                if feature_names_file:\n",
    "                    try:\n",
    "                        with open(feature_names_file, 'rb') as f:\n",
    "                            feature_names = joblib.load(f)\n",
    "                        print(f\"Feature-Namen erfolgreich geladen: {feature_names}\")\n",
    "\n",
    "                        # Konvertiere in DataFrame und entferne nicht-numerische Spalten\n",
    "                        X_model_transformed = pd.DataFrame(X_model_transformed, columns=feature_names)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Fehler beim Laden oder Anwenden der Feature-Namen für '{model_name}': {e}\")\n",
    "                        results[model_name] = (pd.DataFrame(), pd.DataFrame())\n",
    "                        continue\n",
    "\n",
    "            # ** Sicherstellen, dass nur numerische Daten für die Vorhersage verwendet werden **\n",
    "            if isinstance(X_model_transformed, pd.DataFrame):\n",
    "                X_model_transformed = X_model_transformed.select_dtypes(include=[np.number])\n",
    "\n",
    "            # ** Vorhersagen berechnen **\n",
    "            try:\n",
    "                if hasattr(model, \"predict_proba\"):\n",
    "                    y_proba = model.predict_proba(X_model_transformed)[:, 1]\n",
    "                elif hasattr(model, \"predict\"):\n",
    "                    y_proba = model.predict(X_model_transformed).flatten()\n",
    "                else:\n",
    "                    print(f\"Das Modell '{model_name}' unterstützt weder 'predict_proba' noch 'predict'.\")\n",
    "                    results[model_name] = (pd.DataFrame(), pd.DataFrame())\n",
    "                    continue\n",
    "            except Exception as e:\n",
    "                print(f\"Fehler bei Vorhersage für Modell '{model_name}': {e}\")\n",
    "                results[model_name] = (pd.DataFrame(), pd.DataFrame())\n",
    "                continue\n",
    "\n",
    "            # ** Ergebnisse sammeln **\n",
    "            df_copy = df.copy()\n",
    "            df_copy[\"Fluktuationswahrscheinlichkeit\"] = y_proba * 100\n",
    "\n",
    "            # Daten für den letzten Monat und das letzte Jahr filtern\n",
    "            df_copy = df_copy[\n",
    "                (df_copy[\"Fluktuation\"] == 0) &\n",
    "                (df_copy[\"Monat\"] == max_month_in_max_year) &\n",
    "                (df_copy[\"Jahr\"] == max_year)\n",
    "                ]\n",
    "\n",
    "            # Kritische Mitarbeiter und Top-15 bestimmen\n",
    "            critical_employees = df_copy[df_copy[\"Fluktuationswahrscheinlichkeit\"] > threshold]\n",
    "            critical_employees = critical_employees.sort_values(by=\"Fluktuationswahrscheinlichkeit\", ascending=False)\n",
    "            critical_employees = critical_employees.drop_duplicates(subset=[\"Mitarbeiter_ID\"], keep=\"first\")\n",
    "\n",
    "            top_15 = critical_employees.head(15)\n",
    "\n",
    "            # Ergebnisse speichern\n",
    "            results[model_name] = (critical_employees, top_15)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Fehler bei Modell '{model_name}': {e}\")\n",
    "            results[model_name] = (pd.DataFrame(), pd.DataFrame())\n",
    "\n",
    "    return results\n",
    "\n",
    "def save_results(data, file_name_base, output_dir):\n",
    "    \"\"\"\n",
    "    Speichert Ergebnisse sowohl als CSV als auch als Excel im angegebenen Verzeichnis.\n",
    "\n",
    "    Args:\n",
    "        data (pd.DataFrame): Die zu speichernden Daten.\n",
    "        file_name_base (str): Basename der Datei (ohne Endung).\n",
    "        output_dir (str): Zielverzeichnis für die Speicherung.\n",
    "    \"\"\"\n",
    "    csv_path = os.path.join(output_dir, f\"{file_name_base}.csv\")\n",
    "    #xlsx_path = os.path.join(output_dir, f\"{file_name_base}.xlsx\")\n",
    "\n",
    "    # Daten exportieren\n",
    "    data.to_csv(csv_path, index=False)\n",
    "    #data.to_excel(xlsx_path, index=False)\n",
    "\n",
    "    # Feedback an den Nutzer\n",
    "    print(f\"\\nDaten '{file_name_base}' wurden erfolgreich gespeichert:\")\n",
    "    print(f\"- CSV: {csv_path}\")\n",
    "    #print(f\"- Excel: {xlsx_path}\")\n",
    "\n",
    "def compare_model_top_employees(file_paths, output_file=\"Outputs/Vergleich_Top_15.csv\"):\n",
    "    \"\"\"\n",
    "    Lädt die Top-15-Mitarbeiter-Dateien für verschiedene Modelle, vergleicht Namen und IDs,\n",
    "    zählt, wie oft eine Kombination aus Name und ID in den verschiedenen Modellen vorkommt,\n",
    "    gibt Fluktuationswahrscheinlichkeiten je Modell in Prozent an und erstellt eine kompakte Zusammenfassung.\n",
    "\n",
    "    Args:\n",
    "        file_paths (dict): Dictionary mit Modellnamen als Key und Datei-Pfaden als Value.\n",
    "        output_file (str): Name der Ausgabedatei, in der der Vergleich gespeichert wird.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Kompakter DataFrame mit Name, ID, Häufigkeiten, Fluktuationswahrscheinlichkeiten (in Prozent) und Übereinstimmung.\n",
    "    \"\"\"\n",
    "    model_data = {}\n",
    "\n",
    "    print(\"Lade Top-15-Mitarbeiter-Dateien...\")\n",
    "    for model, file_path in file_paths.items():\n",
    "        try:\n",
    "            print(f\"Lade Datei für Modell '{model}' aus {file_path}...\")\n",
    "            df = pd.read_csv(file_path)\n",
    "\n",
    "            # Prüfe notwendige Spalten\n",
    "            required_columns = {\"Name\", \"Mitarbeiter_ID\", \"Fluktuationswahrscheinlichkeit\"}\n",
    "            if not required_columns.issubset(df.columns):\n",
    "                raise ValueError(\n",
    "                    f\"Die Datei '{file_path}' enthält nicht die nötigen Spalten: {required_columns}.\"\n",
    "                )\n",
    "\n",
    "            # Wähle nur benötigte Spalten aus\n",
    "            df = df[[\"Name\", \"Mitarbeiter_ID\", \"Fluktuationswahrscheinlichkeit\"]].copy()\n",
    "\n",
    "            # Spalte mit Modellinformationen umbenennen\n",
    "            df.rename(columns={\"Fluktuationswahrscheinlichkeit\": f\"Fluktuation_{model} (%)\"}, inplace=True)\n",
    "            model_data[model] = df\n",
    "        except Exception as e:\n",
    "            print(f\"Fehler beim Laden von {file_path}: {e}\")\n",
    "            continue\n",
    "\n",
    "    # Kombiniere alle geladenen Daten\n",
    "    print(\"\\nKombiniere Daten aus allen Modellen...\")\n",
    "    combined_df = None\n",
    "    for model, df in model_data.items():\n",
    "        if combined_df is None:\n",
    "            combined_df = df\n",
    "        else:\n",
    "            # Merge basierend auf Name und Mitarbeiter_ID\n",
    "            combined_df = pd.merge(combined_df, df, on=[\"Name\", \"Mitarbeiter_ID\"], how=\"outer\")\n",
    "\n",
    "    # Zähle Übereinstimmungen\n",
    "    print(\"\\nZähle Übereinstimmungen...\")\n",
    "    combined_df[\"Häufigkeit_in_Modellen\"] = combined_df.notna().sum(axis=1) - 2  # Ignoriere 'Name' und 'Mitarbeiter_ID'\n",
    "\n",
    "    # Überprüfen auf gleiche Einträge in mehreren Modellen\n",
    "    combined_df[\"Übereinstimmung\"] = combined_df[\"Häufigkeit_in_Modellen\"] > 1\n",
    "\n",
    "    # Sortiere die Daten alphabetisch nach Name und ID\n",
    "    combined_df = combined_df.sort_values(by=[\"Name\", \"Mitarbeiter_ID\"]).reset_index(drop=True)\n",
    "\n",
    "    # Speichere die Ergebnisse\n",
    "    print(f\"Speichere die Ergebnisse in '{output_file}'...\")\n",
    "    try:\n",
    "        combined_df.to_csv(output_file, index=False)\n",
    "        print(\"Speicherprozess abgeschlossen.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Fehler beim Speichern: {e}\")\n",
    "\n",
    "    return combined_df\n",
    "\n",
    "# Hauptlogik für Fluktuationsanalyse und Modelltraining\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Hauptlogik für die Fluktuationsanalyse und das Modelltraining.\n",
    "    \"\"\"\n",
    "    plot_dir = \"Plots\"\n",
    "    models_dir = \"Models\"\n",
    "    output_dir = \"Outputs\"\n",
    "    output_dir_all = \"Outputs/all_models\"\n",
    "    os.makedirs(plot_dir, exist_ok=True)  # Verzeichnis für Plots erstellen\n",
    "    os.makedirs(models_dir, exist_ok=True)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    os.makedirs(output_dir_all, exist_ok=True)\n",
    "    # Verzeichnis erstellen, falls nicht vorhanden\n",
    "\n",
    "    # 1. Daten laden und vorbereiten\n",
    "    print(\"\\n### Schritt 1: Daten laden ###\")\n",
    "    print(\"Daten laden...\")\n",
    "    file_path = \"HR_cleaned.csv\"\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"Fehler: Die Datei '{file_path}' wurde nicht gefunden.\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        df = load_data(file_path)\n",
    "        print(\"Daten laden abgeschlossen.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Fehler beim Laden der Daten: {e}\")\n",
    "        return\n",
    "\n",
    "    # 2. Daten vorverarbeiten (Preprocessing: One-Hot-Encoding und SMOTE)\n",
    "    # Funktion entfernt 'Status', erstellt 'Fluktuation'\n",
    "    print(\"\\n### Schritt 2: Daten vorverarbeiten ###\")\n",
    "    print(\"Daten vorverarbeiten...\")\n",
    "    df, X_transformed, X_resampled, y_resampled, preprocessor = preprocess_data(df)\n",
    "    print(f\"Shape von X_transformed: {X_transformed.shape}\")\n",
    "    print(f\"Shape von df (Originaldaten): {df.shape}\")\n",
    "    print(\"Preprocessing abgeschlossen.\")\n",
    "\n",
    "    # 3. Daten aufteilen und skalieren (Training/Test-Sets erstellen und skalieren)\n",
    "    print(\"\\n### Schritt 3: Daten aufteilen und skalieren ###\")\n",
    "    print(\"Daten aufteilen und skalieren...\")\n",
    "    X_train, X_test, y_train, y_test, scaler = split_and_scale(X_resampled, y_resampled)\n",
    "    print(f\"Shape von X_train_scaled: {X_train.shape}\")\n",
    "    print(f\"Shape von X_test_scaled: {X_test.shape}\")\n",
    "    print(\"Datenaufteilung und Skalierung abgeschlossen.\")\n",
    "\n",
    "    # 4. PCA-Dimensionalitätsreduktion anwenden (auf Trainings-, Test- und Gesamtdaten)\n",
    "    print(\"\\n### Schritt 4: PCA-Dimensionalitätsreduktion ###\")\n",
    "    pca_file = os.path.join(models_dir, \"pca_model.pkl\")\n",
    "    if os.path.exists(pca_file):\n",
    "        print(\"Gespeichertes PCA-Modell gefunden. Laden...\")\n",
    "        pca = joblib.load(pca_file)\n",
    "\n",
    "        # Transformieren der existierenden Daten\n",
    "        X_train_pca = pca.transform(X_train)\n",
    "        X_test_pca = pca.transform(X_test)\n",
    "        X_full_pca = pca.transform(X_resampled)\n",
    "\n",
    "    else:\n",
    "        print(\"PCA-Dimensionalitätsreduktion durchführen...\")\n",
    "        X_train_pca, X_test_pca, X_full_pca, pca = reduce_dimensions(X_train, X_test, X_resampled)\n",
    "\n",
    "        # Speichern der PCA-Transformation\n",
    "        joblib.dump(pca, pca_file)\n",
    "        print(f\"PCA-Modell gespeichert: {pca_file}\")\n",
    "    print(f\"Shape nach PCA: {X_full_pca.shape}\")\n",
    "    print(\"PCA abgeschlossen.\")\n",
    "\n",
    "    # 5. Modellauswahl basierend auf den Anforderungen\n",
    "    print(\"\\n### Schritt 5: Modellauswahl ###\")\n",
    "    print(\"Modellauswahl durchführen...\")\n",
    "    include_models = model_selection()  # Die Funktion bestimmt, welche Modelle trainiert werden sollen\n",
    "    include_models_all = include_models\n",
    "    if len(include_models) == 1:\n",
    "        print(f\"Ausgewähltes Modell: {include_models[0]}\")\n",
    "    else:\n",
    "        print(f\"Ausgewählte Modelle: {', '.join(include_models)}\")\n",
    "\n",
    "    #6. Benutzerwahl abfragen\n",
    "    print(\"\\n### Schritt 6 Wählen: Gespeicherte Modelle oder neues Training ###\")\n",
    "    use_saved_models = get_user_choice(models_dir)\n",
    "\n",
    "    # 7. Modelle trainieren basierend auf der Auswahl\n",
    "    print(\"\\n### Schritt 7: Modelle trainieren ###\")\n",
    "    print(\"Modelle trainieren...\")\n",
    "\n",
    "    # Modelle trainieren oder laden\n",
    "    trained_models_results = train_models(\n",
    "        X_train, X_test, y_train, y_test, include_models, use_saved_models, models_dir)\n",
    "\n",
    "    # Sicherstellen, dass Ergebnisse verfügbar sind\n",
    "    if not trained_models_results or len(trained_models_results) == 0:\n",
    "        # Überprüfen, ob Modelle überhaupt existieren\n",
    "        if use_saved_models:\n",
    "            print(\"WARNUNG: Gespeicherte Modelle wurden angefordert, aber es gibt keine gespeicherten Modelle.\")\n",
    "            print(\"Starte neues Training...\")\n",
    "            use_saved_models = False  # Umschalten auf Training neuer Modelle\n",
    "            trained_models_results = train_models(\n",
    "                X_train_pca, X_test_pca, y_train, y_test, include_models, use_saved_models, models_dir)\n",
    "\n",
    "        # Nach erneutem Versuch prüfen, ob Training erfolgreich war\n",
    "        if not trained_models_results or len(trained_models_results) == 0:\n",
    "            print(\"FEHLER: Keine Modelle wurden erfolgreich trainiert oder geladen. Programm wird beendet.\")\n",
    "            return\n",
    "\n",
    "    # Extrahieren der Modelle in ein übersichtliches Dictionary\n",
    "    models = {}\n",
    "    y_probas = {}  # Zum Speichern der Wahrscheinlichkeiten\n",
    "    nn_model = None  # Haltemechanismus für Neural Network-Modell, falls vorhanden\n",
    "\n",
    "    for result in trained_models_results:\n",
    "        # Model spezifisch prüfen\n",
    "        model_name = result.get(\"model_name\")\n",
    "        trained_model = result.get(\"trained_model\")\n",
    "        roc_auc_score_nn = result.get(\"y_test_proba\", None)\n",
    "\n",
    "        # Speichern des Modells\n",
    "        if model_name and trained_model:\n",
    "            models[model_name] = trained_model\n",
    "\n",
    "        # Für Neural Network Wahrscheinlichkeiten speziell speichern\n",
    "        if model_name == \"Neural Network\":\n",
    "            nn_model = trained_model  # Setzt das neuronale Modell\n",
    "            if roc_auc_score_nn is not None:\n",
    "                y_probas[\"Neural Network\"] = result[\"y_test_proba\"]  # Speichern der ROC-Wahrscheinlichkeit\n",
    "        else:\n",
    "            y_probas[model_name] = result.get(\"roc_auc_score\")  # Für andere Modelle prüfen: ROC direkt speichern\n",
    "\n",
    "    # Prüfen, ob überhaupt ein Modell trainiert wurde\n",
    "    if not models:\n",
    "        print(\"WARNUNG: Keine Modelle stehen für die Analyse bereit.\")\n",
    "        return  # Kein Modell - Abbruch\n",
    "\n",
    "    print(f\"{len(models)} Modelle erfolgreich trainiert.\")\n",
    "\n",
    "    # 8. Analyse von Overfitting/Underfitting\n",
    "    print(\"\\n### Schritt 8: Over-/Underfitting-Analyse und Plots erstellen ###\")\n",
    "\n",
    "    # Variablen definieren\n",
    "    model_names = [result[\"model_name\"] for result in trained_models_results]  # Namen der Modelle\n",
    "    train_accuracies = [result[\"train_accuracy\"] for result in trained_models_results]  # Trainingsgenauigkeiten\n",
    "    test_accuracies = [result[\"test_accuracy\"] for result in trained_models_results]  # Testgenauigkeiten\n",
    "    roc_auc_scores = [result[\"roc_auc_score\"] for result in trained_models_results]  # ROC AUC-Werte\n",
    "\n",
    "    # Zusätzliche Metriken optional extrahieren\n",
    "    f1_scores = [result.get(\"f1_score\", None) for result in trained_models_results]  # Optional: F1-Scores\n",
    "    precisions = [result.get(\"precision\", None) for result in trained_models_results]  # Optional: Präzision\n",
    "    recalls = [result.get(\"recall\", None) for result in trained_models_results]  # Optional: Recall-Werte\n",
    "\n",
    "\n",
    "    # Horizontales Balkendiagramm für Modellvergleiche erstellen\n",
    "    print(print(\"\\nHorizontales Balkendiagramm für Modellvergleiche erstellen ...\"))\n",
    "    plot_horizontal_comparison(\n",
    "        model_names=model_names,\n",
    "        train_accuracies=train_accuracies,\n",
    "        test_accuracies=test_accuracies,\n",
    "        roc_auc_scores=roc_auc_scores,\n",
    "        f1_scores=f1_scores,\n",
    "        precisions=precisions,\n",
    "        recalls=recalls,\n",
    "        plot_dir=plot_dir,\n",
    "        sort_by=\"Test Accuracy\",  # Sortieren nach Testgenauigkeit\n",
    "        include_train=True,  # Trainingsgenauigkeiten einbeziehen\n",
    "        xlabel=\"Model Performance (Higher is Better)\"\n",
    "    )\n",
    "    print(\"Horizontale Plots wurden erfolgreich erstellt.\")\n",
    "\n",
    "    # Over-/Underfitting testen und Ergebnisse visualisieren\n",
    "    print(\"\\nAnalyse von Overfitting und Underfitting ...\")\n",
    "    fit_results = test_under_over_fit(model_names, train_accuracies, test_accuracies)\n",
    "    plot_under_over_fit(model_names, train_accuracies, test_accuracies, plot_dir)\n",
    "    print(\"Over-/Underfitting-Ergebnisse:\")\n",
    "    print(fit_results)\n",
    "\n",
    "\n",
    "\n",
    "    # 9. ROC-Kurven für alle Modelle erstellen\n",
    "    print(\"\\n### Schritt 9: ROC-Kurven erstellen ###\")\n",
    "    print(\"ROC-Kurven erstellen...\")\n",
    "    try:\n",
    "        # Dictionary `models` enthält alle trainierten Modelle\n",
    "        # Dictionary `y_probas` enthält die Wahrscheinlichkeiten\n",
    "\n",
    "        # Aufruf der ROC-Plot-Funktion\n",
    "        plot_combined_roc_curves(\n",
    "            models=models,\n",
    "            X_test=X_test,\n",
    "            y_test=y_test,\n",
    "            nn_model=nn_model,  # Das neuronale Netzwerk-Modell\n",
    "            y_proba_nn=y_probas.get(\"Neural Network\", None),  # Wahrscheinlichkeiten für nn\n",
    "            plot_dir=plot_dir\n",
    "        )\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Fehler bei der Erstellung der ROC-Kurven: {e}\")\n",
    "\n",
    "    # 10. Modelle bewerten\n",
    "    print(\"\\n### Schritt 10: Modellbewertung ###\")\n",
    "    print(\"Modelle bewerten...\")\n",
    "    # Übergabe der PCA-Daten zusätzlich an die Funktion\n",
    "    evaluation_results = evaluate_models(\n",
    "        models=models,\n",
    "        X_test=X_test,  # Original-Testdaten\n",
    "        y_test=y_test,  # Wahre Labels\n",
    "        plot_dir=plot_dir  # Verzeichnis für Plots\n",
    "    )\n",
    "\n",
    "    # 11. Bestes Modell auswählen\n",
    "    print(\"\\n### Schritt 11: Modellauswahl ###\")\n",
    "    print(\"\\nBestes Modell auswählen...\")\n",
    "    try:\n",
    "        best_model, best_model_name = get_best_model(models, evaluation_results, fit_results, primary_metric=\"ROC-AUC\",\n",
    "                                                     tie_breaker=\"F1-Score\")\n",
    "        print(f\"\\nDas beste Modell ist: {best_model_name}\")\n",
    "    except ValueError as e:\n",
    "        print(f\"Fehler bei der Modellauswahl: {e}\")\n",
    "        return\n",
    "\n",
    "    # 12. Kritische Mitarbeiter und Top 15 ermitteln\n",
    "    print(\"\\n### Schritt 12: Kritische Mitarbeiter und Top 15 ermitteln ###\")\n",
    "    print(\"Kritische Mitarbeiter und Top 15 ermitteln...\")\n",
    "    try:\n",
    "        # Überprüfen, ob PCA angewendet werden soll (z. B. für die logistische Regression)\n",
    "        apply_pca = best_model_name == \"Logistic Regression\"  # PCA nur bei log. Regression anwenden\n",
    "\n",
    "        # Funktionsaufruf, mit oder ohne PCA, abhängig vom Modell\n",
    "        critical_employees, top_15_mitarbeiter = get_critical_employees(\n",
    "            best_model,\n",
    "            X_transformed,\n",
    "            df,\n",
    "            pca=pca if apply_pca else None  # PCA nur übergeben, wenn logistische Regression\n",
    "        )\n",
    "\n",
    "        if critical_employees.empty:\n",
    "            print(\"WARNUNG: Keine kritischen Mitarbeiter gefunden (keine Wahrscheinlichkeiten > 70%).\")\n",
    "        else:\n",
    "            print(f\"Anzahl der kritischen Mitarbeiter: {len(critical_employees)}\")\n",
    "\n",
    "        if top_15_mitarbeiter.empty:\n",
    "            print(\"WARNUNG: Keine Top 15 Mitarbeiter gefunden.\")\n",
    "        else:\n",
    "            print(\n",
    "                f\"Top 15 Mitarbeiter:\\n{top_15_mitarbeiter[['Mitarbeiter_ID', 'Name', 'Fluktuationswahrscheinlichkeit']]}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Fehler beim Ermitteln der kritischen Mitarbeiter: {e}\")\n",
    "        return\n",
    "\n",
    "    # 13. Kritische Mitarbeiter und Top 15 für jedes Modell ermitteln\n",
    "    print(\"\\n### Schritt 13: Kritische Mitarbeiter und Top 15 für jedes Modell ermitteln ###\")\n",
    "    print(\"Kritische Mitarbeiter und Top 15 ermitteln...\")\n",
    "\n",
    "    try:\n",
    "        for model_name in include_models_all:  # Verwende die Modellnamen aus der Auswahl\n",
    "            try:\n",
    "                print(f\"\\nBearbeite Modell: {model_name}\")\n",
    "\n",
    "                apply_pca = model_name == \"Logistic Regression\"  # Prüfen, ob PCA erforderlich ist\n",
    "\n",
    "                # Funktionsaufruf für kritische Mitarbeiter\n",
    "                # Übergib die Modelle als Dictionary mit nur dem aktuellen Modell\n",
    "                critical_employees_data = get_critical_employees_all_models(\n",
    "                    models={model_name: models[model_name]},  # Nur das aktuelle Modell verarbeiten\n",
    "                    X_transformed=X_transformed,\n",
    "                    df=df,\n",
    "                    pca=pca if apply_pca else None\n",
    "                )\n",
    "\n",
    "                # Ergebnisse extrahieren\n",
    "                if model_name in critical_employees_data:\n",
    "                    critical_employees, top_15_employees = critical_employees_data[model_name]\n",
    "\n",
    "                    # Ergebnisse validieren und anzeigen\n",
    "                    if critical_employees.empty:\n",
    "                        print(f\"WARNUNG: Keine kritischen Mitarbeiter für {model_name} gefunden.\")\n",
    "                    else:\n",
    "                        print(f\"Anzahl der kritischen Mitarbeiter für {model_name}: {len(critical_employees)}\")\n",
    "                        print(critical_employees.head(5))\n",
    "\n",
    "                    if top_15_employees.empty:\n",
    "                        print(f\"WARNUNG: Keine Top 15 Mitarbeiter für {model_name} gefunden.\")\n",
    "                    else:\n",
    "                        print(f\"Top 15 Mitarbeiter für {model_name}:\\n\"\n",
    "                              f\"{top_15_employees[['Mitarbeiter_ID', 'Name', 'Fluktuationswahrscheinlichkeit']]}\")\n",
    "\n",
    "                    # Ergebnisse speichern\n",
    "                    save_results(\n",
    "                        data=critical_employees,\n",
    "                        file_name_base=f\"Critical_Mitarbeiter_{model_name.replace(' ', '_')}\",\n",
    "                        output_dir=output_dir_all\n",
    "                    )\n",
    "                    save_results(\n",
    "                        data=top_15_employees,\n",
    "                        file_name_base=f\"Top_15_Mitarbeiter_{model_name.replace(' ', '_')}\",\n",
    "                        output_dir=output_dir_all\n",
    "                    )\n",
    "                else:\n",
    "                    print(f\"WARNUNG: Keine Ergebnisse für {model_name} vorhanden.\")\n",
    "\n",
    "            except KeyError:\n",
    "                print(f\"FEHLER: Modell '{model_name}' wurde nicht in 'models' gefunden.\")\n",
    "            except Exception as e:\n",
    "                print(f\"Fehler bei der Verarbeitung des Modells '{model_name}': {e}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Fehler beim Ermitteln der kritischen Mitarbeiter und Top 15 für alle Modelle: {e}\")\n",
    "\n",
    "    # 13. Ergebnisse speichern\n",
    "    print(\"\\n### Schritt 13: Ergebnisse speichern ###\")\n",
    "    print(\"Ergebnisse speichern...\")\n",
    "    save_results(critical_employees, \"Critical_Mitarbeiter\", output_dir)\n",
    "    save_results(top_15_mitarbeiter, \"Top_15_Mitarbeiter\", output_dir)\n",
    "\n",
    "    print(\"### Komprimiert: Komprimiert\")\n",
    "    print(\"### Programm beendet.\")\n",
    "\n",
    "    #14. Vergleich der Modelle\n",
    "    print(\"\\n### Schritt 14: Vergleich der Modelle ###\")\n",
    "    file_paths = {\n",
    "        \"LightGBM\": \"Outputs/all_models/Top_15_Mitarbeiter_LightGBM.csv\",\n",
    "        \"Logistic_Regression\": \"Outputs/all_models/Top_15_Mitarbeiter_Logistic_Regression.csv\",\n",
    "        \"Neural_Network\": \"Outputs/all_models/Top_15_Mitarbeiter_Neural_Network.csv\",\n",
    "        \"Random_Forest\": \"Outputs/all_models/Top_15_Mitarbeiter_Random_Forest.csv\",\n",
    "        \"XGBoost\": \"Outputs/all_models/Top_15_Mitarbeiter_XGBoost.csv\"\n",
    "    }\n",
    "\n",
    "    result = compare_model_top_employees(file_paths)\n",
    "    print(result)\n",
    "\n",
    "\n",
    "# Ausführung der Hauptfunktion\n",
    "if __name__ == \"__main__\":\n",
    "    warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "    start_time = time.time()\n",
    "    main()\n",
    "    plt.close(\"all\")\n",
    "    k.clear_session()\n",
    "    end_time = time.time()\n",
    "    print(f\"\\nAnalyse abgeschlossen in {end_time - start_time:.2f} Sekunden.\")"
   ],
   "id": "c0d1467d73e8fd6"
  }
 ],
 "metadata": {},
 "nbformat": 5,
 "nbformat_minor": 9
}
